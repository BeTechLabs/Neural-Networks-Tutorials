<pre>
Archive-name: ai-faq/neural-nets/part7
Last-modified: 2002-04-09
URL: ftp://ftp.sas.com/pub/neural/FAQ7.html
Maintainer: saswss@unx.sas.com (Warren S. Sarle)
</pre>
<title>Neural Network FAQ, part 7 of 7: Hardware and Miscellaneous</title>

Copyright 1997, 1998, 1999, 2000, 2001, 2002 by Warren S. Sarle, Cary, NC, USA. Answers provided
by other authors as cited below are copyrighted by those authors, who by
submitting the answers for the FAQ give permission for the answer to be
reproduced as part of the FAQ in any of the ways specified in part 1 of
the FAQ.  <p>

This is part 7 (of 7) of a monthly posting to the Usenet newsgroup
comp.ai.neural-nets. See the part 1 of this posting for full
information what it is all about.<p>

<H1><a name="questions">
========== Questions ==========
</a></H1>

<a href="FAQ.html#questions">
Part 1: Introduction</a><br>

<a href="FAQ2.html#questions">
Part 2: Learning</a><br>

<a href="FAQ3.html#questions">
Part 3: Generalization</a><br>

<a href="FAQ4.html#questions">
Part 4: Books, data, etc.</a><br>

<a href="FAQ5.html#questions">
Part 5: Free software</a><br>

<a href="FAQ6.html#questions">
Part 6: Commercial software</a><br>

Part 7: Hardware and miscellaneous<br>
<OL>
 <a href="FAQ7.html#A20">Neural Network hardware?</a><br>
 <a href="FAQ7.html#A_applications">What are some applications of NNs?</a><br>
<OL>
    <a href="FAQ7.html#A_app_general">General</a><br>
    <a href="FAQ7.html#A_app_agriculture">Agriculture</a><br>
    <a href="FAQ7.html#A_app_auto">Automotive</a><br>
    <a href="FAQ7.html#A_app_chemistry">Chemistry</a><br>
    <a href="FAQ7.html#A_app_crime">Criminology</a><br>
    <a href="FAQ7.html#A_app_face">Face recognition</a><br>
    <a href="FAQ7.html#A_app_finance">Finance and economics</a><br>
    <a href="FAQ7.html#A_app_games">Games, sports, gambling</a><br>
    <a href="FAQ7.html#A_app_industry">Industry</a><br>
    <a href="FAQ7.html#A_app_materials">Materials science</a><br>
    <a href="FAQ7.html#A_app_medicine">Medicine</a><br>
    <a href="FAQ7.html#A_app_music">Music</a><br>
    <a href="FAQ7.html#A_app_robot">Robotics</a><br>
    <a href="FAQ7.html#A_app_weather">Weather forecasting</a><br>
    <a href="FAQ7.html#A_app_weird">Weird</a><br>
</OL>
 <a href="FAQ7.html#A_missing">What to do with missing/incomplete data?</a><br>
 <a href="FAQ7.html#A_forecast">How to forecast time series (temporal sequences)?</a><br>
 <a href="FAQ7.html#A_inverse">How to learn an inverse of a function?</a><br>
 <a href="FAQ7.html#A_invariant">How to get invariant recognition of
    images under translation, rotation, etc.?</a><br>
 <a href="FAQ7.html#A_handwrite">How to recognize handwritten characters?</a><br>
 <a href="FAQ7.html#A_spike">What about pulsed or spiking NNs?</a><br>
 <a href="FAQ7.html#A_genetic">What about Genetic Algorithms and Evolutionary Computation?</a><br>
 <a href="FAQ7.html#A_fuzzy">What about Fuzzy Logic?</a><br>
 <a href="FAQ7.html#A_un">Unanswered FAQs</a><br>
 <a href="FAQ7.html#A_links">Other NN links?</a><br>
</OL>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A20">Neural Network hardware?</a></H2>

Overview articles:
<ul>
<li>
Ienne, Paolo and Kuhn, Gary (1995), "Digital Systems for Neural
Networks", in Papamichalis, P. and Kerwin, R., eds., 
<cite>Digital Signal Processing Technology,</cite> 
Critical Reviews Series CR57
Orlando, FL: SPIE Optical Engineering, pp 314-45,
<a href="ftp://mantraftp.epfl.ch/mantra/ienne.spie95.A4.ps.gz">
ftp://mantraftp.epfl.ch/mantra/ienne.spie95.A4.ps.gz</a> or
<a href="ftp://mantraftp.epfl.ch/mantra/ienne.spie95.US.ps.gz">
ftp://mantraftp.epfl.ch/mantra/ienne.spie95.US.ps.gz</a> 
<li>
<A HREF="ftp://ftp.mrc-apu.cam.ac.uk/pub/nn/murre/neurhard.ps">
ftp://ftp.mrc-apu.cam.ac.uk/pub/nn/murre/neurhard.ps</A> (1995)
<li>
<A HREF="ftp://ftp.urc.tue.nl/pub/neural/hardware_general.ps.gz">
ftp://ftp.urc.tue.nl/pub/neural/hardware_general.ps.gz</A> (1993)
</ul>

Various NN HW information can be found in the Web site
<A HREF="http://www1.cern.ch/NeuralNets/nnwInHepHard.html">
http://www1.cern.ch/NeuralNets/nnwInHepHard.html</A> (from people
who really use such stuff!). Several applications are described in
<A HREF="http://www1.cern.ch/NeuralNets/nnwInHepExpt.html">
http://www1.cern.ch/NeuralNets/nnwInHepExpt.html</A> <P>

More information on NN chips can be obtained from the Electronic 
Engineers Toolbox web page. Go to
<a href="http://www.eg3.com/ebox.htm">http://www.eg3.com/ebox.htm,</a>
type "neural" in the quick search box, click on "chip co's" and then
on "search". <p>

Further WWW pointers to NN Hardware:
<ul>
<li><A HREF="http://msia02.msi.se/~lindsey/nnwAtm.html">
    http://msia02.msi.se/~lindsey/nnwAtm.html</A>
<li><A HREF="http://www.genobyte.com/article.html">
    http://www.genobyte.com/article.html</A>
</ul>
<p>

Here is a short list of companies:
<ol>
<li><H3> HNC, INC.</H3>
<pre>
  HNC Inc.
  5930 Cornerstone Court West
  San Diego, CA 92121-3728

  619-546-8877  Phone
  619-452-6524  Fax
</pre>
  HNC markets:<ul>
<li>Database Mining Workstation (DMW), a PC based system that
   builds models of relationships and patterns in data.
<li>The SIMD Numerical Array Processor (SNAP). It is an attached
   parallel array processor in a VME chassis with between 16 and 64 parallel
   floating point processors. It provides between 640 MFLOPS and 2.56 GFLOPS
   for neural network and signal processing applications.  A Sun SPARCstation
   serves as the host.  The SNAP won the IEEE 1993 Gordon Bell Prize for best
   price/performance for supercomputer class systems.
</ul>
<p>

<li><H3> SAIC (Sience Application International Corporation)</H3>
<pre>
   10260 Campus Point Drive
   MS 71, San Diego
   CA 92121
   (619) 546 6148
   Fax: (619) 546 6736
</pre><p>

<li><H3> Micro Devices</H3>
<pre>
   30 Skyline Drive
   Lake Mary
   FL 32746-6201
   (407) 333-4379
</pre>
   MicroDevices makes MD1220 - 'Neural Bit Slice'.
   Each of the products mentioned sofar have very different usages.
   Although this sounds similar to Intel's product, the
   architectures are not.
<p>

<li><H3> Intel Corp</H3>
<pre>
   2250 Mission College Blvd
   Santa Clara, Ca 95052-8125
   Attn ETANN, Mail Stop SC9-40
   (408) 765-9235
</pre>
   Intel was making an experimental chip (which is no longer produced):
   80170NW - Electrically trainable Analog Neural Network (ETANN)
   It has 64 'neurons' on it - almost fully internally connectted
   and the chip can be put in an hierarchial architecture to do 2 Billion
   interconnects per second.
   Support software by
<pre>
     California Scientific Software
     10141 Evening Star Dr #6
     Grass Valley, CA 95945-9051
     (916) 477-7481
</pre>
   Their product is called 'BrainMaker'.
<p>

<li><H3> Tubb Research Limited</H3>
<pre>
   7a Lavant Street
   Peterfield
   Hampshire
   GU32 2EL
   United Kingdom
   Tel: +44 730 60256
</pre><p>

<li><H3> Adaptive Solutions Inc</H3>
<pre>
   1400 NW Compton Drive
   Suite 340
   Beaverton, OR 97006
   U. S. A.
   Tel: 503-690-1236;   FAX: 503-690-1249
</pre><p>

<li><H3> NeuroDynamX, Inc.</H3>
<pre>
   P.O. Box 14
   Marion, OH  43301-0014
   Voice (740) 387-5074
   Fax: (740) 382-4533
   Internet:  jwrogers@on-ramp.net
   <a href="http://www.neurodynamx.com">http://www.neurodynamx.com</a>
</pre>
   InfoTech Software Engineering purchased the software and
   trademarks from NeuroDynamX, Inc. and, using the NeuroDynamX tradename,
   continues to publish the DynaMind, DynaMind Developer Pro and iDynaMind
   software packages.
<p>

<li><H3> NeuroClassifier </H3>

URL: <a href="http://www.ice.el.utwente.nl/Finished/Neuro/">http://www.ice.el.utwente.nl/Finished/Neuro/</a><br>
Email: peter.masa@csemne.ch <p>

<li><H3> NeuriCam, S.r.l.</H3>
<pre>
   Via S. Maria Maddalena, 38100 Trento, Italy
   Tel: +39 0461 260 552
   Fax: +39 0461 260 617
   Email: info@neuricam.com
</pre>
NC3001 TOTEM - Digital Processor for Neural Networks<br>
TOTEM is a digital VLSI parallel processor for fast learning and
recognition with artificial neural networks. Its high processing power,
low power dissipation and limited chip size make it ideally suited for
embedded applications. The architecture is optimised for the
implementation of the Reactive Tabu Search learning algorithm, a
competitive alternative to back-propagation which leads to a very
compact VLSI implementation.
<p>

</ol>

And here is an incomplete overview of known Neural Computers with
their newest known reference.<p>

<ul>
<li>Special Computers<p>

<ol>

<li> AAP-2
Takumi Watanabe, Yoshi Sugiyama, Toshio Kondo, and Yoshihiro Kitamura.<br>
"Neural network simulation on a massively parallel cellular array
processor: AAP-2."
In International Joint Conference on Neural Networks, 1989.

<li> ANNA
B.E.Boser, E.Sackinger, J.Bromley, Y.leChun, and L.D.Jackel.<br>
"Hardware Requirements for Neural Network Pattern Classifiers."
In <cite> IEEE Micro</cite>, 12(1), pages 32-40, February 1992.

<li> Analog Neural Computer
Paul Mueller et al.<br>
"Design and performance of a prototype analog neural computer."
In Neurocomputing, 4(6):311-323, 1992.

<li> APx -- Array Processor Accelerator
F.Pazienti.<br>
"Neural networks simulation with array processors."
In <cite> Advanced Computer Technology, Reliable Systems and Applications;
Proceedings of the 5th Annual Computer Conference</cite>, pages 547-551.
IEEE Comput. Soc. Press, May 1991. ISBN: 0-8186-2141-9.

<li> ASP -- Associative String Processor
A.Krikelis.<br>
"A novel massively associative processing architecture for the
implementation artificial neural networks."
In <cite> 1991 International Conference on Acoustics, Speech and
Signal Processing</cite>, volume 2, pages 1057-1060. IEEE Comput. Soc. Press,
May 1991.

<li> BSP400
Jan N.H. Heemskerk, Jacob M.J. Murre, Jaap Hoekstra, Leon H.J.G.
Kemna, and Patrick T.W. Hudson.<br>
"The bsp400: A modular neurocomputer assembled from 400 low-cost
microprocessors."
In International Conference on Artificial Neural Networks. Elsevier
Science, 1991.

<li> BLAST
J.G.Elias, M.D.Fisher, and C.M.Monemi.<br>
"A multiprocessor machine for large-scale neural network simulation."
In <cite> IJCNN91-Seattle: International Joint Conference on Neural
Networks</cite>, volume 1, pages 469-474. IEEE Comput. Soc. Press, July 1991.
ISBN: 0-7883-0164-1.

<li> CNAPS Neurocomputer
H.McCartor<br>
"Back Propagation Implementation on the Adaptive Solutions CNAPS
Neurocomputer."
In <cite> Advances in Neural Information Processing Systems</cite>, 3, 1991.

<li> GENES~IV and MANTRA~I
Paolo Ienne and  Marc A. Viredaz<br>
"GENES~IV: A Bit-Serial Processing Element for a Multi-Model
Neural-Network Accelerator."
Journal of VLSI Signal Processing, volume 9, no. 3, pages 257--273, 1995.

<li> MA16 -- Neural Signal Processor
U.Ramacher, J.Beichter, and N.Bruls.<br>
"Architecture of a general-purpose neural signal processor."
In <cite> IJCNN91-Seattle: International Joint Conference on Neural
Networks</cite>, volume 1, pages 443-446. IEEE Comput. Soc. Press, July 1991.
ISBN: 0-7083-0164-1.

<li> Mindshape
Jan N.H. Heemskerk, Jacob M.J. Murre Arend Melissant, Mirko Pelgrom,
and Patrick T.W. Hudson.<br>
"Mindshape: a neurocomputer concept based on a fractal architecture."
In International Conference on Artificial Neural Networks. Elsevier
Science, 1992.

<li> mod 2
Michael L. Mumford, David K. Andes, and Lynn R. Kern.<br>
"The mod 2 neurocomputer system design."
In IEEE Transactions on Neural Networks, 3(3):423-433, 1992.

<li> NERV
R.Hauser, H.Horner, R. Maenner, and M.Makhaniok.<br>
"Architectural Considerations for NERV - a General Purpose Neural
Network Simulation System."
In <cite> Workshop on Parallel Processing: Logic, Organization and
Technology -- WOPPLOT 89</cite>, pages 183-195. Springer Verlag, Mars 1989.
ISBN: 3-5405-5027-5.

<li> NP -- Neural Processor
D.A.Orrey, D.J.Myers, and J.M.Vincent.<br>
"A high performance digital processor for implementing large artificial
neural networks."
In <cite> Proceedings of of the IEEE 1991 Custom Integrated Circuits
Conference</cite>, pages 16.3/1-4. IEEE Comput. Soc. Press, May 1991.
ISBN: 0-7883-0015-7.

<li> RAP -- Ring Array Processor 
N.Morgan, J.Beck, P.Kohn, J.Bilmes, E.Allman, and J.Beer.<br>
"The ring array processor: A multiprocessing peripheral for connectionist
applications."
In <cite> Journal of Parallel and Distributed Computing</cite>, pages
248-259, April 1992.

<li> RENNS -- REconfigurable Neural Networks Server
O.Landsverk, J.Greipsland, J.A.Mathisen, J.G.Solheim, and L.Utne.<br>
"RENNS - a Reconfigurable Computer System for Simulating Artificial
Neural Network Algorithms."
In <cite> Parallel and Distributed Computing Systems, Proceedings of the
ISMM 5th International Conference</cite>, pages 251-256. The International
Society for Mini and Microcomputers - ISMM, October 1992.
ISBN: 1-8808-4302-1.

<li> SMART -- Sparse Matrix Adaptive and Recursive Transforms
P.Bessiere, A.Chams, A.Guerin, J.Herault, C.Jutten, and J.C.Lawson.<br>
"From Hardware to Software: Designing a `Neurostation'."
In <cite> VLSI design of Neural Networks</cite>, pages 311-335, June 1990.

<li> SNAP -- Scalable Neurocomputer Array Processor
E.Wojciechowski.<br>
"SNAP: A parallel processor for implementing real time neural networks."
In <cite> Proceedings of the IEEE 1991 National Aerospace and Electronics
Conference; NAECON-91</cite>, volume 2, pages 736-742. IEEE Comput.Soc.Press,
May 1991.

<li> Toroidal Neural Network Processor
S.Jones, K.Sammut, C.Nielsen, and J.Staunstrup.<br>
"Toroidal Neural Network: Architecture and Processor Granularity
Issues."
In <cite> VLSI design of Neural Networks</cite>, pages 229-254, June 1990.

<li> SMART and SuperNode
P. Bessi`ere, A. Chams, and P. Chol.<br>
"MENTAL : A virtual machine approach to artificial neural networks
programming." In NERVES, ESPRIT B.R.A. project no 3049, 1991.

</ol>

<p>
<li>Standard Computers<p>

<ol>

<li> EMMA-2
R.Battiti, L.M.Briano, R.Cecinati, A.M.Colla, and P.Guido.<br>
"An application oriented development environment for Neural Net models on
multiprocessor Emma-2."
In <cite> Silicon Architectures for Neural Nets; Proceedings for the IFIP
WG.10.5 Workshop</cite>, pages 31-43. North Holland, November 1991.
ISBN: 0-4448-9113-7.

<li> iPSC/860 Hypercube
D.Jackson, and D.Hammerstrom<br>
"Distributing Back Propagation Networks Over the Intel iPSC/860
Hypercube"
In <cite> IJCNN91-Seattle: International Joint Conference on Neural
Networks</cite>, volume 1, pages 569-574. IEEE Comput. Soc. Press, July 1991.
ISBN: 0-7083-0164-1.

<li> SCAP -- Systolic/Cellular Array Processor
Wei-Ling L., V.K.Prasanna, and K.W.Przytula.<br>
"Algorithmic Mapping of Neural Network Models onto Parallel SIMD
Machines."
In <cite> IEEE Transactions on Computers</cite>, 40(12), pages 1390-1401,
December 1991. ISSN: 0018-9340.

</ol>

</ul>

</ol>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_applications">What are some applications of NNs?</a></H2>

There are vast numbers of published neural network applications.
If you don't find something from your field of interest below,
try a web search. Here are some useful search engines:<br>
<a href="http://www.google.com/">http://www.google.com/</a><br>
<a href="http://search.yahoo.com/">http://search.yahoo.com/</a><br>
<a href="http://www.altavista.com/">http://www.altavista.com/</a><br>
<a href="http://www.deja.com/">http://www.deja.com/</a><p>

<h4><a name="A_app_general">General</a></h4>
<ul>
    <li>The Pacific Northwest National Laboratory:
        <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/">http://www.emsl.pnl.gov:2080/proj/neuron/neural/</a>
        including a list of commercial applications at
        <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/products/">http://www.emsl.pnl.gov:2080/proj/neuron/neural/products/</a>
    <li>The Stimulation Initiative for European Neural Applications:
        <a href="http://www.mbfys.kun.nl/snn/siena/cases/">http://www.mbfys.kun.nl/snn/siena/cases/</a>
    <li>The DTI NeuroComputing Web's Applications Portfolio:
        <a href="http://www.globalweb.co.uk/nctt/portfolo/">http://www.globalweb.co.uk/nctt/portfolo/</a>
    <li>The Applications Corner, NeuroDimension, Inc.:
        <a href="http://www.nd.com/appcornr/purpose.htm">http://www.nd.com/appcornr/purpose.htm</a>
    <li>The BioComp Systems, Inc. Solutions page:
        <a href="http://www.bio-comp.com/solution.htm">http://www.bio-comp.com</a>
    <li>Chen, C.H., ed. (1996) <cite>Fuzzy Logic and Neural Network Handbook,</cite>
        NY: McGraw-Hill, ISBN 0-07-011189-8.
    <li>The series <cite>Advances in Neural Information Processing
        Systems</cite> containing proceedings of the conference of
        the same name, published yearly by Morgan Kauffman starting in 1989
        and by The MIT Press in 1995.
</ul>


<h4><a name="A_app_agriculture">Agriculture</a></h4>
<ul>
<li>P.H. Heinemann, Automated Grading of Produce:
    <a href="http://server.age.psu.edu/dept/fac/Heinemann/phhdocs/visionres.html">http://server.age.psu.edu/dept/fac/Heinemann/phhdocs/visionres.html</a><br>
<li>Deck, S., C.T. Morrow, P.H. Heinemann, and H.J. Sommer, III. 1995.
    Comparison of a neural network and traditional classifier for machine
    vision inspection. Applied Engineering in Agriculture. 11(2):319-326.
<li>Tao, Y., P.H. Heinemann, Z. Varghese, C.T. Morrow, and H.J. Sommer III.
    1995. Machine vision for color inspection of potatoes and apples.
    Transactions of the American Society of Agricultural Engineers.
    38(5):1555-1561.
</ul>

<h4><a name="A_app_auto">Automotive</a></h4>
<ul>
<li>"No Hands Across America Journal" - steering a car:
    <a href="http://cart.frc.ri.cmu.edu/users/hpm/project.archive/reference.file/Journal.html">
    http://cart.frc.ri.cmu.edu/users/hpm/project.archive/reference.file/Journal.html</a><br>
    Photos: <a href="http://www.techfak.uni-bielefeld.de/ags/ti/personen/zhang/seminar/intelligente-autos/tour.html">
    http://www.techfak.uni-bielefeld.de/ags/ti/personen/zhang/seminar/intelligente-autos/tour.html</a><br>
</ul>

<h4><a name="A_app_chemistry">Chemistry</a></h4>
<ul>
<li>PNNL, General Applications of Neural Networks in Chemistry and
    Chemical Engineering:
    <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/chemistry.html">http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/chemistry.html.</a>
<li>Prof. Dr. Johann Gasteiger, Neural Networks and Genetic Algorithms in Chemistry:
    <a href="http://www2.ccc.uni-erlangen.de/publications/publ_topics/publ_topics-12.html">http://www2.ccc.uni-erlangen.de/publications/publ_topics/publ_topics-12.html</a>
<li>Roy Goodacre, pyrolysis mass spectrometry:
    <a href="http://gepasi.dbs.aber.ac.uk/roy/pymshome.htm">http://gepasi.dbs.aber.ac.uk/roy/pymshome.htm</a>
    and Fourier transform infrared (FT-IR) spectroscopy:
    <a href="http://gepasi.dbs.aber.ac.uk/roy/ftir/ftirhome.htm">http://gepasi.dbs.aber.ac.uk/roy/ftir/ftirhome.htm</a>
    contain applications of a variety of NNs as well as PLS (partial
    least squares) and other statistical methods.
<li>Situs, a program package for the docking of protein crystal
    structures to single-molecule, low-resolution maps from electron
    microscopy or small angle X-ray scattering:
    <a href="http://chemcca10.ucsd.edu/~situs/">http://chemcca10.ucsd.edu/~situs/</a>
<li>An on-line application of a Kohonen network with a 2-dimensional
    output layer for prediction of protein secondary structure
    percentages from UV circular dichroism spectra:
    <a href="http://www.embl-heidelberg.de/~andrade/k2d/">http://www.embl-heidelberg.de/~andrade/k2d/</a>.
</ul>

<h4><a name="A_app_crime">Criminology</a></h4>
<ul>
    <li>Computer Aided Tracking and Characterization of Homicides and
        Sexual Assaults (CATCH):
        <a href="http://lancair.emsl.pnl.gov:2080/proj/neuron/papers/kangas.spie99.abs.html">http://lancair.emsl.pnl.gov:2080/proj/neuron/papers/kangas.spie99.abs.html</a>
</ul>

<h4><a name="A_app_face">Face recognition</a></h4>
<ul>
    <li>Face Recognition Home Page:
        <a href="http://www.cs.rug.nl/~peterkr/FACE/face.html">http://www.cs.rug.nl/~peterkr/FACE/face.html</a>
    <li>Konen, W., "Neural information processing in real-world face-recognition applications,"
        <a href="http://www.computer.muni.cz/pubs/expert/1996/trends/x4004/konen.htm">http://www.computer.muni.cz/pubs/expert/1996/trends/x4004/konen.htm</a>
    <li>Jiang, Q., "Principal Component Analysis and Neural Network
        Based Face Recognition,"
        <a href="http://people.cs.uchicago.edu/~qingj/ThesisHtml/">http://people.cs.uchicago.edu/~qingj/ThesisHtml/</a>
    <li>Lawrence, S., Giles, C.L., Tsoi, A.C., Back, A.D. (1997),
        "Face Recognition: A Convolutional Neural Network Approach,"
        IEEE Transactions on Neural Networks, 8, 98-113,
        <a href="http://www.neci.nec.com/~lawrence/papers/face-tnn97/latex.html">http://www.neci.nec.com/~lawrence/papers/face-tnn97/latex.html</a>
</ul>

<h4><a name="A_app_finance">Finance and economics</a></h4>
<ul>
    <li>Athanasios Episcopos, References on Neural Net 
        Applications to Finance and Economics:
        <a href="http://www.compulink.gr/users/episcopo/neurofin.html">http://www.compulink.gr/users/episcopo/neurofin.html</a>
        <!episcopo@fire.camp.clarkson.edu>
    <li>Franco Busetti, Heuristics and artificial intelligence in finance and investment:
        <a href="http://www.geocities.com/francorbusetti/">http://www.geocities.com/francorbusetti/</a>
        <!francob@iafrica.com>
    <li>Trippi, R.R. & Turban, E. (1993), <cite>Neural Networks in Finance and
        Investing,</cite> Chicago: Probus.
    <li>Zirilli, J.S. (1996), <cite>Financial Prediction Using Neural Networks,</cite>
        International Thomson Publishing, ISBN 1850322341, 
        <a href="http://www6.bcity.com/mjfutures/">http://www6.bcity.com/mjfutures/</a>
    <li>Andreas S. Weigend, Yaser Abu-Mostafa, A. Paul N. Refenes (eds.) (1997)
        <cite>Decision Technologies for Financial Engineering: Proceedings of the Fourth International
        Conference on Neural Networks in the Capital Markets</cite> (Nncm '96)
        Publisher: World Scientific Publishing Company, ISBN: 9810231245
              
</ul>


<h4><a name="A_app_games">Games, sports, gambling</a></h4>
<ul>
<li>General:<p>

    Jay Scott, Machine Learning in Games:
    <a href="http://satirist.org/learn-game/index.html">http://satirist.org/learn-game/index.html</a><p>

    METAGAME Game-Playing Workbench:
    <a href="ftp://ftp.cl.cam.ac.uk/users/bdp/">ftp://ftp.cl.cam.ac.uk/users/bdp/METAGAME</a><p>

    R.S. Sutton, "Learning to predict by the methods of temporal
    differences", Machine Learning 3, p. 9-44 (1988). <p>

    David E. Moriarty and Risto Miikkulainen (1994). "Evolving Neural Networks to Focus
    Minimax Search," In Proceedings of Twelfth National Conference on Artificial
    Intelligence (AAAI-94, Seattle, WA), 1371-1377. Cambridge, MA: MIT Press,
    <a href="http://www.cs.utexas.edu/users/nn/pages/publications/neuro-evolution.html">http://www.cs.utexas.edu/users/nn/pages/publications/neuro-evolution.html</a><p>

    Games World '99 at <a href="http://gamesworld99.free.fr/menuframe.htm">http://gamesworld99.free.fr/menuframe.htm</a><p>

<li>Backgammon:<p>

    G. Tesauro and T.J. Sejnowski (1989),
    "A Parallel Network that learns to play Backgammon,"
    Artificial Intelligence, vol 39, pp. 357-390. <p>
 
    G. Tesauro and T.J. Sejnowski (1990),
    "Neurogammon: A Neural Network Backgammon Program,"
    IJCNN Proceedings, vol 3, pp. 33-39, 1990. <p>

    G. Tesauro (1995), "Temporal Difference Learning and TD-Gammon,"
    Communications of the ACM, 38, 58-68,
    <a href="http://www.research.ibm.com/massive/tdl.html">http://www.research.ibm.com/massive/tdl.html</a> <p>

    Pollack, J.P. and Blair, A.D. (1997),
    "Co-Evolution in the Successful Learning of Backgammon Strategy," 
    Brandeis University Computer Science Technical Report CS-97-193,
    <a href="http://www.demo.cs.brandeis.edu/papers/long.html#hcgam97">http://www.demo.cs.brandeis.edu/papers/long.html#hcgam97</a><p>

<li>Bridge:<p>

    METAGAME:
    <a href="ftp://ftp.cl.cam.ac.uk/users/bdp/">ftp://ftp.cl.cam.ac.uk/users/bdp/bridge.ps.Z</a><p>

    He Yo, Zhen Xianjun, Ye Yizheng, Li Zhongrong (19??),
    "Knowledge acquisition and reasoning based on neural networks -
       the research of a bridge bidding system,"
    INNC '90, Paris, vol 1, pp. 416-423. <p>
 
    M. Kohle and F. Schonbauer (19??),
    "Experience gained with a neural network that learns to play bridge,"
    Proc. of the 5th Austrian Artificial Intelligence meeting, pp.
    224-229. <p>

<li><a name="A_app_games_checkers">Checkers/Draughts:</a> <p> 

    Mark Lynch (1997), "NeuroDraughts: an application of temporal
    difference learning to draughts,"
    <a href="http://www.ai.univie.ac.at/~juffi/lig/Papers/lynch-thesis.ps.gz">http://www.ai.univie.ac.at/~juffi/lig/Papers/lynch-thesis.ps.gz</a>
    Software available at
    <a href="http://satirist.org/learn-game/archive/NeuroDraughts-1.00.zip">http://satirist.org/learn-game/archive/NeuroDraughts-1.00.zip</a><p>
    <p>

    K. Chellapilla and D. B. Fogel, "Co-Evolving Checkers Playing 
    Programs using Only Win, Lose, or Draw," SPIE's AeroSense'99:
    Applications and Science of Computational Intelligence II,
    Apr. 5-9, 1999, Orlando, Florida, USA,
    <a href="http://vision.ucsd.edu/~kchellap/Publications.html">http://vision.ucsd.edu/~kchellap/Publications.html</a>
    <p>

    David Fogel (1999), <cite>Evolutionary Computation: Toward
    a New Philosophy of Machine Intelligence</cite> (2nd edition),
    IEEE, ISBN: 078035379X <p>

    David Fogel (2001), <cite>Blondie24: Playing at the Edge of AI,</cite>
    Morgan Kaufmann Publishers, ISBN: 1558607838<br>
    According to the publisher, this is:
    <blockquote>
    ... the first book to bring together
    the most advanced work in the general use of evolutionary
    computation for creative results. It is well suited for the general
    computer science audience.
     <p>
    Here's the story of a computer that taught itself to play checkers
    far better than its creators ever could. Blondie24 uses a program
    that emulates the basic principles of Darwin evolution to discover
    on its own how to excel at the game. Through this entertaining
    story, the book provides the reader some of the history of AI and
    explores its future.
    <p>
    Unlike Deep Blue, the celebrated chess machine that beat Garry
    Kasparov, the former world champion chess player, this evolutionary
    program didn't have access to other games played by human grand
    masters, or databases of moves for the endgame. It created its own
    means for evaluating the patterns of pieces that it experienced by
    evolving artificial neural networks--mathematical models that
    loosely describe how a brain works.
    </blockquote>
    <p>

    See <a href="http://www.natural-selection.com/NSIPublicationsOnline.htm">http://www.natural-selection.com/NSIPublicationsOnline.htm</a>
    for a variety of online papers by Fogel. <p>

    Not NNs, but classic papers:<p>

    A.L. Samuel (1959),
    "Some studies in machine learning using the game of checkers,"
    IBM journal of Research and Development, vol 3, nr. 3, pp. 210-229. <p>
 
    A.L. Samuel (1967),
    "Some studies in machine learning using the game of checkers 2 -
       recent progress,"
    IBM journal of Research and Development, vol 11, nr. 6, pp. 601-616. <p>

<li>Chess:<p>

    Sebastian Thrun, NeuroChess: 
    <a href="http://satirist.org/learn-game/systems/neurochess.html">http://satirist.org/learn-game/systems/neurochess.html</a><p>

    Luke Pellen, Octavius:
    <a href="http://home.seol.net.au/luke/octavius/">http://home.seol.net.au/luke/octavius/</a><p>

    Louis Savain (AKA Nemesis), Animal, a spiking neural network that
    the author hopes will learn to play a passable game of chess
    after he implements the motivation mechanism:
    <a href="http://home1.gte.net/res02khr/AI/Temporal_Intelligence.htm">http://home1.gte.net/res02khr/AI/Temporal_Intelligence.htm</a><p>

<li>Dog racing:<p>

    H. Chen1, P. Buntin, L. She, S. Sutjahjo, C. Sommer, D. Neely (1994),
    "Expert Prediction, Symbolic Learning, and Neural Networks: 
    An Experiment on Greyhound Racing," IEEE Expert, December 1994, 21-27,
    <a href="http://ai.bpa.arizona.edu/papers/dog93/dog93.html">http://ai.bpa.arizona.edu/papers/dog93/dog93.html</a><p>

<li>Football (Soccer):<p>

    Kuonen Diego, "Statistical Models for Knock-out Soccer Tournaments",
    <a href="http://dmawww.epfl.ch/~kuonen/CALCIO/">http://dmawww.epfl.ch/~kuonen/CALCIO/</a>
    (not neural nets, but relevant)<p>

<li>Go:<p>

    David Stoutamire (19??),
    "Machine Learning, Game Play, and Go,"
    Center for Automation and Intelligent Systems Research TR 91-128, 
    Case Western Reserve University. 
    <a href="http://www.stoutamire.com/david/publications.html">http://www.stoutamire.com/david/publications.html</a>
    <p>

    David Stoutamire (1991),
    <cite>Machine Learning Applied to Go,</cite>
    M.S. thesis, Case Western Reserve University,
    <a href="ftp://ftp.cl.cam.ac.uk/users/bdp/">ftp://ftp.cl.cam.ac.uk/users/bdp/go.ps.Z</a>
    <p>

    Schraudolph, N., Dayan, P., Sejnowski, T. (1994), "Temporal
    Difference Learning of Position Evaluation in the Game of Go,"
    In: Neural Information Processing Systems 6, Morgan Kaufmann 1994,
    <a href="ftp://bsdserver.ucsf.edu/Go/comp/td-go.ps.Z">ftp://bsdserver.ucsf.edu/Go/comp/td-go.ps.Z</a>
    <p>

    P. Donnelly, P. Corr & D. Crookes (1994), "Evolving Go Playing Strategy
    in Neural Networks", AISB Workshop on Evolutionary Computing, Leeds,
    England,
    <a href="ftp://www.joy.ne.jp/welcome/igs/Go/computer/egpsnn.ps.Z">ftp://www.joy.ne.jp/welcome/igs/Go/computer/egpsnn.ps.Z</a> or
    <a href="ftp://ftp.cs.cuhk.hk/pub/neuro/GO/techreports/egpsnn.ps.Z">ftp://ftp.cs.cuhk.hk/pub/neuro/GO/techreports/egpsnn.ps.Z</a>
    <p>

    Markus Enzenberger (1996), "The Integration of A Priori Knowledge
    into a Go Playing Neural Network,"
    <a href="http://www.cgl.ucsf.edu/go/Programs/neurogo-html/neurogo.html">http://www.cgl.ucsf.edu/go/Programs/neurogo-html/neurogo.html</a>
    <p>

    Norman Richards, David Moriarty, and Risto Miikkulainen (1998),
    "Evolving Neural Networks to Play Go," Applied Intelligence, 8, 85-96,
    <a href="http://www.cs.utexas.edu/users/nn/pages/publications/neuro-evolution.html">http://www.cs.utexas.edu/users/nn/pages/publications/neuro-evolution.html</a>
    <p>

    Dahl, F. A. (1999), "Honte, a Go-playing program using neural nets",
    <a href="http://www.ai.univie.ac.at/icml-99-ws-games/papers/dahl.ps.gz">http://www.ai.univie.ac.at/icml-99-ws-games/papers/dahl.ps.gz</a>
    <p>  

<li>Go-Moku:<p>

    Freisleben, B., "Teaching a Neural Network to Play GO-MOKU," in
    I. Aleksander and J. Taylor, eds, Artificial Neural Networks 2,
    Proc. of ICANN-92, Brighton UK, vol. 2, pp. 1659-1662,
    Elsevier Science Publishers, 1992 <p>

    Katz, W.T. and Pham, S.P.  "Experience-Based Learning Experiments
    using Go-moku", Proc. of the 1991 IEEE International Conference on
    Systems, Man, and Cybernetics, 2: 1405-1410, October 1991. <p>

<li>Olympics:<p>

    E.M.Condon, B.L.Golden, E.A.Wasil (1999),
    "Predicting the success of nations at the Summer Olympics
    using neural networks",
    Computers & Operations Research, 26, 1243-1265. <p>

<li>Pong:<p>

    <a href="http://www.engin.umd.umich.edu/~watta/MM/pong/pong5.html">http:// www.engin.umd.umich.edu/~watta/MM/pong/pong5.html</a> <p>

<li>Reversi/Othello:<p>

    David E. Moriarty and Risto Miikkulainen (1995). Discovering
    Complex Othello Strategies through Evolutionary Neural Networks.
    Connection Science, 7, 195-209,
    <a href="http://www.cs.utexas.edu/users/nn/pages/publications/neuro-evolution.html">http://www.cs.utexas.edu/users/nn/pages/publications/neuro-evolution.html</a>
    <p>

    Yoshioka, T., Ishii, S., and Ito, M.,
    Strategy acquisition for the game ``Othello'' based on reinforcement learning,
    IEICE Transactions on Information and Systems E82-D 12, 1618-1626, 1999,
    <a href="http://mimi.aist-nara.ac.jp/~taku-y/">http://mimi.aist-nara.ac.jp/~taku-y/</a>
    <p>


<li>Tic-Tac-Toe/Noughts and Crosses:<p>

    Fogel, David Bb (1993), "Using evolutionary programming to
    construct neural networks that are capable of playing tic-tac-toe,"
    Intern. Conf. on Neural Networks 1993, IEEE, San Francisco, CA,
    pp. 875-880. <p>

    Richard S. Sutton and Andrew G. Barto (1998),
    <cite>Reinforcement Learning: An Introduction</cite>
    The MIT Press, ISBN: 0262193981,
    <a href="http://www-anw.cs.umass.edu/~rich/book/the-book.html">http://www-anw.cs.umass.edu/~rich/book/the-book.html</a>
    <p>

    Yongzheng Zhang, Chen Teng, Sitan Wei (2000), "Game playing with
    Evolutionary Strategies and Modular Neural Networks: Tic-Tac-Toe,"
    <a href="http://www.cs.dal.ca/~mheywood/GAPproject/EvolvingGamePlay.html">http://www.cs.dal.ca/~mheywood/GAPproject/EvolvingGamePlay.html</a>
    <p>

    Rob Ellison, "Neural Os and Xs,"
    <a href="http://www.catfood.demon.co.uk/beta/game.html">http://www.catfood.demon.co.uk/beta/game.html</a>
    (An online Javascript demo, but you may not live long enough to
    teach the network to play a mediocre game. I'm not sure what kind
    of network it uses, but maybe you can figure that out if you read
    the source.) <p>

    <a href=""></a>
    
    <a href=""></a>

</ul>


<h4><a name="A_app_industry">Industry</a></h4>
<ul>
<li>PNNL, Neural Network Applications in Manufacturing:
    <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/manufacturing.html">http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/manufacturing.html.</a>
<li>PNNL, Applications in the Electric Power Industry:
    <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/power.html">http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/power.html.</a>
<li>PNNL, Process Control:
    <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/process.html">http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/process.html.</a>
<li>Raoul Tawel, Ken Marko, and Lee Feldkamp (1998),
    "Custom VLSI ASIC for Automotive Applications with Recurrent 
    Networks",
    <a href="http://www.jpl.nasa.gov/releases/98/ijcnn98.pdf ">http://www.jpl.nasa.gov/releases/98/ijcnn98.pdf </a><br>
<li>Otsuka, Y. et al.
  "Neural Networks and Pattern Recognition of Blast Furnace Operation Data"
  Kobelco Technology Review, Oct. 1992, 12
<li>Otsuka, Y. et al.
  "Applications of Neural Network to Iron and Steel Making Processes"
  2. International Conference on Fuzzy Logic and Neural Networks, Iizuka, 
  1992
<li>Staib, W.E.
  "Neural Network Control System for Electric Arc Furnaces"
  M.P.T. International, 2/1995, 58-61
<li>Portmann, N. et al.
  "Application of Neural Networks in Rolling Automation"
  Iron and Steel Engineer, Feb. 1995, 33-36
<li>Gorni, A.A. (2000), "The modelling of hot rolling processes using
  neural networks: A bibliographical review",
  <a href="http://www.geocities.com/SiliconValley/5978/neural_1998.html">http://www.geocities.com/SiliconValley/5978/neural_1998.html</a>
<li>Murat, M. E., and Rudman, A. J., 1992, Automated first arrival picking: A
  neural network approach: Geophysical Prospecting, 40, 587-604.
</ul>

<h4><a name="A_app_materials">Materials science</a></h4>
<ul>
<li>Phase Transformations Research Group (search for "neural"):
    <a href="http://www.msm.cam.ac.uk/phase-trans/pubs/ptpuball.html">http://www.msm.cam.ac.uk/phase-trans/pubs/ptpuball.html</a>
</ul>

<h4><a name="A_app_medicine">Medicine</a></h4>
<ul>
<li>PNNL, Applications in Medicine and Health: 
    <a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/medicine.html">http://www.emsl.pnl.gov:2080/proj/neuron/neural/bib/medicine.html.</a>
</ul>

<h4><a name="A_app_music">Music</a></h4>
<ul>
<li>Mozer, M. C. (1994), "Neural network music composition
    by prediction: Exploring the benefits of psychophysical
    constraints and multiscale processing," Connection Science,
    6, 247-280,
    <a href="http://www.cs.colorado.edu/~mozer/papers/music.html">http://www.cs.colorado.edu/~mozer/papers/music.html.</a>
<li>Griffith, N., and Todd, P.M., eds. (1999), <cite>Musical Networks:
    Parallel Distributed Perception and Performance,</cite> Cambridge, MA:
    The MIT Press, ISBN 0-262-07181-9.
</ul>

<h4><a name="A_app_robot">Robotics</a></h4>
<ul>
<li>Institute of Robotics and System Dynamics:
    <a href="http://www.robotic.dlr.de/LEARNING/">http://www.robotic.dlr.de/LEARNING/</a>
<li>UC Berkeley Robotics and Intelligent Machines Lab:
    <a href="http://robotics.eecs.berkeley.edu/">http://robotics.eecs.berkeley.edu/</a>
<li>Perth Robotics and Automation Laboratory:
    <a href="http://telerobot.mech.uwa.edu.au/">http://telerobot.mech.uwa.edu.au/</a>
<li>University of New Hampshire Robot Lab:
    <a href="http://www.ece.unh.edu/robots/rbt_home.htm">http://www.ece.unh.edu/robots/rbt_home.htm</a> 
</ul>


<h4><a name="A_app_weather">Weather forecasting and atmospheric science</a></h4>
<ul>
<li>UBC Climate Prediction Group:
    <a href="http://www.ocgy.ubc.ca/projects/clim.pred/index.html">http://www.ocgy.ubc.ca/projects/clim.pred/index.html</a>
<li>Artificial Intelligence Research In Environmental Science:
    <a href="http://www.salinas.net/~jpeak/airies/airies.html">http://www.salinas.net/~jpeak/airies/airies.html</a>
<li>MET-AI, an mailing list for meteorologists and AI researchers:
    <a href="http://www.comp.vuw.ac.nz/Research/met-ai">http://www.comp.vuw.ac.nz/Research/met-ai</a>
<li>Caren Marzban, Ph.D., Research Scientist, National Severe Storms Laboratory:
    <a href="http://www.nhn.ou.edu/~marzban/">http://www.nhn.ou.edu/~marzban/</a>
<li>David Myers's references on NNs in atmospheric science:
    <a href="http://terra.msrc.sunysb.edu/~dmyers/ai_refs">http://terra.msrc.sunysb.edu/~dmyers/ai_refs</a>
</ul>


<h4><a name="A_app_weird">Weird</a></h4>

<! http://ciips.ee.uwa.edu.au/~tonko/abstracts.html>

Zaknich, Anthony and Baker, Sue K. (1998),
"A real-time system for the characterisation of sheep feeding phases
from acoustic signals of jaw sounds,"
Australian Journal of Intelligent Information Processing Systems
(AJIIPS), Vol. 5, No. 2, Winter 1998. <p>

Abstract<br>
This paper describes a four-channel real-time system for the detection
and measurement of sheep rumination and mastication time periods by the
analysis of jaw sounds transmitted through the skull. The system is
implemented using an 80486 personal computer, a proprietary data
acquisition card (PC-126) and a custom made variable gain preamplifier
and bandpass filter module. Chewing sounds are transduced and
transmitted to the system using radio microphones attached to the top of
the sheep heads. The system's main functions are to detect and estimate
rumination and mastication time periods, to estimate the number of chews
during the rumination and mastication periods, and to provide estimates
of the number of boli in the rumination sequences and the number of
chews per bolus. The individual chews are identified using a special
energy threshold detector. The rumination and mastication time periods
are determined by neural network classifier using a combination of time
and frequency domain features extracted from successive 10 second
acoustic signal blocks.
<p>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_missing">What to do with missing/incomplete data?
</a></H2>

The problem of missing data is very complex. <p>

For unsupervised learning, conventional statistical methods for missing
data are often appropriate (Little and Rubin, 1987; Schafer, 1997).
There is a concise introduction to these methods in the University of
Texas statistics FAQ at

<a href="http://www.utexas.edu/cc/faqs/stat/general/gen25.html">http://www.utexas.edu/cc/faqs/stat/general/gen25.html.</a>
<p>

For supervised learning, the considerations are somewhat different, as
discussed by Sarle (1998).  The statistical literature on missing data
deals almost exclusively with training rather than prediction (e.g.,
Little, 1992). For example, if you have only a small proportion of cases
with missing data, you can simply throw those cases out for purposes of
training; if you want to make predictions for cases with missing inputs,
you don't have the option of throwing those cases out!  In theory,
Bayesian methods take care of everything, but a full Bayesian analysis
is practical only with special models (such as multivariate normal
distributions) or small sample sizes.  The neural net literature
contains a few good papers that cover prediction with missing inputs
(e.g., Ghahramani and Jordan, 1997; Tresp, Neuneier, and Ahmad 1995),
but much research remains to be done. <p>

References:
<dl>
<dt><dd><p>
   Donner, A. (1982), "The relative effectiveness of procedures
   commonly used in multiple regression analysis for dealing with
   missing values," American Statistician, 36, 378-381.
<dt><dd><p>
   Ghahramani, Z. and Jordan, M.I. (1994), "Supervised learning from
   incomplete data via an EM approach," in Cowan, J.D., Tesauro, G.,
   and Alspector, J. (eds.) <cite>Advances in Neural Information
   Processing Systems 6,</cite> San Mateo, CA: Morgan Kaufman, pp. 120-127.
<dt><dd><p>
   Ghahramani, Z. and Jordan, M.I. (1997), "Mixture models for
   Learning from incomplete data," in Greiner, R., Petsche, T., and
   Hanson, S.J. (eds.) <cite>Computational Learning Theory and Natural
   Learning Systems, Volume IV: Making Learning Systems Practical,</cite>
   Cambridge, MA: The MIT Press, pp. 67-85.
<dt><dd><p>
   Jones, M.P. (1996), "Indicator and stratification methods for
   missing explanatory variables in multiple linear regression,"
   J. of the American Statistical Association, 91, 222-230.
<dt><dd><p>
   Little, R.J.A. (1992), "Regression with missing X's: A review,"
   J. of the American Statistical Association, 87, 1227-1237.
<dt><dd><p>
   Little, R.J.A. and Rubin, D.B. (1987), <cite>Statistical Analysis 
   with Missing Data,</cite> NY: Wiley.
<dt><dd><p>
   McLachlan, G.J. (1992) <cite>Discriminant Analysis and Statistical Pattern
   Recognition,</cite> Wiley.
<dt><dd><p>
   Sarle, W.S. (1998), "Prediction with Missing Inputs," in
   Wang, P.P. (ed.), JCIS '98 Proceedings, Vol II, 
   Research Triangle Park, NC, 399-402,
   <a href="ftp://ftp.sas.com/pub/neural/JCIS98.ps">ftp://ftp.sas.com/pub/neural/JCIS98.ps.</a>
<dt><dd><p>
   Schafer, J.L. (1997), <cite>Analysis of Incomplete Multivariate
   Data,</cite> London: Chapman & Hall, ISBN 0 412 04061 1.
<dt><dd><p>
   Tresp, V., Ahmad, S. and Neuneier, R., (1994), "Training neural
   networks with deficient data", in Cowan, J.D., Tesauro, G., and
   Alspector, J. (eds.) <cite>Advances in Neural Information Processing
   Systems 6,</cite> San Mateo, CA: Morgan Kaufman, pp. 128-135.
<dt><dd><p>
   Tresp, V., Neuneier, R., and Ahmad, S. (1995), "Efficient methods
   for dealing with missing data in supervised learning", in 
   Tesauro, G., Touretzky, D.S., and Leen, T.K. (eds.) <cite>Advances
   in Neural Information Processing Systems 7,</cite> Cambridge, MA:
   The MIT Press, pp. 689-696.
</dl>


<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_forecast">How to forecast time series (temporal sequences)?
</a></H2>

In most of this FAQ, it is assumed that the training cases are
statistically independent. That is, the training cases consist of pairs
of input and target vectors, <tt>(X_i,Y_i), i=1,...,N,</tt> such that
the conditional distribution of <tt>Y_i</tt> given all the other
training data, (<tt>X_j, j=1,...,N,</tt> and
<tt>Y_j, j=1,...i-1,i+1,...N</tt>) is equal to the conditional
distribution of <tt>Y_i</tt> given <tt>X_i</tt> regardless of the values
in the other training cases. Independence of cases is often achieved by
random sampling. <p>

The most common violation of the independence assumption occurs when
cases are observed in a certain order relating to time or space. That
is, case <tt>(X_i,Y_i)</tt> corresponds to time <tt>T_i,</tt> with
<tt>T_1 &lt; T_2 &lt; ... &lt; T_N.</tt> It is assumed that the current
target <tt>Y_i</tt> may depend not only on <tt>X_i</tt> but also on
<tt>(X_i,Y_i)</tt> in the recent past. If the <tt>T_i</tt> are equally
spaced, the simplest way to deal with this dependence is to include
additional inputs (called lagged variables, shift registers, or a tapped
delay line) in the network. Thus, for target <tt>Y_i,</tt> the inputs
may include <tt>X_i,</tt> <tt>Y_{i-1}, X_{i-1}, Y_{i-1}, X_{i-2},</tt>
etc.  (In some situations, <tt>X_i</tt> would not be known at the time
you are trying to forecast <tt>Y_i</tt> and would therefore be excluded
from the inputs.) Then you can train an ordinary feedforward network
with these targets and lagged variables.  The use of lagged variables
has been extensively studied in the statistical and econometric
literature (Judge, Griffiths, Hill, L&uuml;tkepohl and Lee, 1985). A
network in which the only inputs are lagged target values is called an
"autoregressive model." The input space that includes all of the lagged
variables is called the "embedding space." <p>

If the <tt>T_i</tt> are <em>not</em> equally spaced, everything gets
much more complicated. One approach is to use a smoothing technique
to interpolate points at equally spaced intervals, and then use the
interpolated values for training instead of the original data. <p>

Use of lagged variables increases the number of decisions that must be
made during training, since you must consider which lags to include in
the network, as well as which input variables, how many hidden units,
etc.  Neural network researchers have therefore attempted to use
partially recurrent networks instead of feedforward networks with lags
(Weigend and Gershenfeld, 1994).  Recurrent networks store information
about past values in the network itself. There are many different kinds
of recurrent architectures (Hertz, Krogh, and Palmer 1991; Mozer, 1994;
Horne and Giles, 1995; Kremer, 199?).  For example, in time-delay neural
networks (Lang, Waibel, and Hinton 1990), the outputs for predicting
target <tt>Y_{i-1}</tt> are used as inputs when processing target
<tt>Y_i.</tt> Jordan networks (Jordan, 1986) are similar to time-delay
neural networks except that the feedback is an exponential smooth of the
sequence of output values.  In Elman networks (Elman, 1990), the hidden
unit activations that occur when processing target <tt>Y_{i-1}</tt> are
used as inputs when processing target <tt>Y_i.</tt> <p>

However, there are some problems that cannot be dealt with via recurrent
networks alone. For example, many time series exhibit trend, meaning
that the target values tend to go up over time, or that the target
values tend to go down over time. For example, stock prices and many
other financial variables usually go up.  If today's price is higher
than all previous prices, and you try to forecast tomorrow's price using
today's price as a lagged input, you are extrapolating, and
extrapolating is unreliable. The simplest methods for handling trend
are:

<ul>

<li>First fit a linear regression predicting the target values
    from the time, <tt>Y_i = a + b T_i + noise,</tt> where <tt>a</tt>
    and <tt>b</tt> are regression weights. Compute residuals
    <tt>R_i = Y_i - (a + b T_i).</tt> Then train the network using
    <tt>R_i</tt> for the target and lagged values.
    This method is rather crude but may work for deterministic
    linear trends. Of course, for nonlinear trends, you would
    need to fit a nonlinear regression. <p>

<li>Instead of using <tt>Y_i</tt> as a target, use <tt>D_i = Y_i -
    Y_{i-1}</tt> for the target and lagged values. This is called
    differencing and is the standard statistical method for handling
    nondeterministic (stochastic) trends. Sometimes it is necessary
    to compute differences of differences. <p>

</ul>

For an elementary discussion of trend and various other practical
problems in forecasting time series with NNs, such as seasonality, see
Masters (1993).  For a more advanced discussion of NN forecasting of
economic series, see Moody (1998).  <p>

There are several different ways to compute forecasts. For simplicity,
let's assume you have a simple time series, <tt>Y_1, ..., Y_99,</tt>
you want to forecast future values <tt>Y_f</tt> for <tt>f &gt; 99,</tt>
and you decide to use three lagged values as inputs.  The possibilities
include:

<dl>

<dt>Single-step, one-step-ahead, or open-loop forecasting:
<dd>Train a network with target <tt>Y_i</tt> and
    inputs <tt>Y_{i-1},</tt> <tt>Y_{i-2},</tt> and <tt>Y_{i-3}.</tt>
    Let the scalar function computed by the network be designated
    as <tt>Net(.,.,.)</tt> taking the three input values as
    arguments and returning the output (predicted) value. Then:<br>
    forecast <tt>Y_100</tt> as <tt>Net(Y_99,Y_98,Y_97)</tt><br>
    forecast <tt>Y_101</tt> as <tt>Net(Y_100,Y_99,Y_98)</tt><br>
    forecast <tt>Y_102</tt> as <tt>Net(Y_101,Y_100,Y_99)</tt><br>
    forecast <tt>Y_103</tt> as <tt>Net(Y_102,Y_101,Y_100)</tt><br>
    forecast <tt>Y_104</tt> as <tt>Net(Y_103,Y_102,Y_101)</tt><br>
    and so on.
<p>

<dt>Multi-step or closed-loop forecasting:
<dd>Train the network as above, but:<br>
    forecast <tt>Y_100</tt> as <tt>P_100 = Net(Y_99,Y_98,Y_97)</tt><br>
    forecast <tt>Y_101</tt> as <tt>P_101 = Net(P_100,Y_99,Y_98)</tt><br>
    forecast <tt>Y_102</tt> as <tt>P_102 = Net(P_101,P_100,Y_99)</tt><br>
    forecast <tt>Y_103</tt> as <tt>P_103 = Net(P_102,P_101,P_100)</tt><br>
    forecast <tt>Y_104</tt> as <tt>P_104 = Net(P_103,P_102,P_101)</tt><br>
    and so on.
<p>

<dt><tt>N</tt>-step-ahead forecasting:
<dd>For, say, <tt>N=3</tt>, train the network as above, but:<br>
    compute <tt>P_100 = Net(Y_99,Y_98,Y_97)</tt><br>
    compute <tt>P_101 = Net(P_100,Y_99,Y_98)</tt><br>
    forecast <tt>Y_102</tt> as <tt>P_102 = Net(P_101,P_100,Y_99)</tt><br>
    forecast <tt>Y_103</tt> as <tt>P_103 = Net(P_102,P_101,Y_100)</tt><br>
    forecast <tt>Y_104</tt> as <tt>P_104 = Net(P_103,P_102,Y_101)</tt><br>
    and so on.
<p>

<dt>Direct simultaneous long-term forecasting:
<dd>Train a network with multiple targets <tt>Y_i,</tt> <tt>Y_{i+1},</tt>
    and <tt>Y_{i+2}</tt>
    and inputs <tt>Y_{i-1},</tt> <tt>Y_{i-2},</tt> and <tt>Y_{i-3}.</tt>
    Let the vector function computed by the network be designated
    as <tt>Net3(.,.,.),</tt> taking
    the three input values as arguments and returning the
    output (predicted) vector. Then:<br>
    forecast <tt>(Y_100,Y_101,Y_102)</tt> as <tt>Net3(Y_99,Y_98,Y_97)</tt><br>
<p>

</dl>

Which method you choose for computing forecasts will obviously depend
in part on the requirements of your application. If you have yearly
sales figures through 1999 and you need to forecast sales in 2003,
you clearly can't use single-step forecasting. If you need to compute
forecasts at a thousand different future times, using direct simultaneous
long-term forecasting would require an extremely large network. <p>

If a time series is a random walk, a well-trained network will predict
<tt>Y_i</tt> by simply outputting <tt>Y_{i-1}.</tt> If you make a plot
showing both the target values and the outputs, the two curves will
almost coincide, except for being offset by one time step. People
often mistakenly intrepret such a plot to indicate good forecasting
accuracy, whereas in fact the network is virtually useless. In such
situations, it is more enlightening to plot multi-step forecasts
or <tt>N</tt>-step-ahead forecasts. <p>

For general information on time-series forecasting, see the following
URLs:
<ul>
<li>Forecasting FAQs: <a href="http://forecasting.cwru.edu/faqs.html">http://forecasting.cwru.edu/faqs.html</a>
<li>Forecasting Principles: <a href="http://hops.wharton.upenn.edu/forecast/">http://hops.wharton.upenn.edu/forecast/</a>
<li>Investment forecasts for stocks and mutual funds: <a href="http://www.coe.uncc.edu/~hphillip/">http://www.coe.uncc.edu/~hphillip/</a>
</ul>

References:
<dl>
<dt><dd><p>
   Elman, J.L. (1990), "Finding structure in time," Cognitive Science,
   14, 179-211. 
<dt><dd><p>
   Hertz, J., Krogh, A., and Palmer, R. (1991). <cite>Introduction to the Theory of
   Neural Computation.</cite> Addison-Wesley: Redwood City, California.
<dt><dd><p>
   Horne, B. G. and Giles, C. L. (1995),
   "An experimental comparison of recurrent neural networks,"
   In Tesauro, G., Touretzky, D., and Leen, T., editors, Advances in Neural Information Processing
   Systems 7, pp. 697-704. The MIT Press. 
<dt><dd><p>
   Jordan, M. I. (1986), 
   "Attractor dynamics and parallelism in a connectionist sequential machine,"
   In Proceedings of the Eighth Annual conference of the Cognitive Science Society, pages 531-546.
   Lawrence Erlbaum. 
<dt><dd><p>
   Judge, G.G., Griffiths, W.E., Hill, R.C., L&uuml;tkepohl, H., and Lee, T.-C.
   (1985), <cite>The Theory and Practice of Econometrics,</cite> NY:
   John Wiley & Sons.
<dt><dd><p>
   Kremer, S.C. (199?), "Spatio-temporal Connectionist Networks: 
   A Taxonomy and Review,"
   <a href="http://hebb.cis.uoguelph.ca/~skremer/Teaching/27642/dynamic2/review.html">http://hebb.cis.uoguelph.ca/~skremer/Teaching/27642/dynamic2/review.html.</a>
<dt><dd><p>
   Lang, K. J., Waibel, A. H., and Hinton, G. (1990), 
   "A time-delay neural network architecture for isolated word recognition,"
   Neural Networks, 3, 23-44. 
<dt><dd><p>
   Masters, T. (1993). <cite>Practical Neural Network Recipes in C++,</cite>
   San Diego: Academic Press.
<dt><dd><p>
   Moody, J. (1998), "Forecasting the economy with neural nets: A
   survey of challenges and solutions," in 
   Orr, G,B., and Mueller, K-R, eds., <cite>Neural Networks:
   Tricks of the Trade,</cite> Berlin: Springer.
<dt><dd><p>
   Mozer, M.C. (1994), "Neural net architectures for temporal
   sequence processing," in Weigend, A.S. and Gershenfeld, N.A., eds.
   (1994) <cite>Time Series Prediction: Forecasting the Future and
   Understanding the Past</cite>, Reading, MA: Addison-Wesley, 243-264,
   <a href="http://www.cs.colorado.edu/~mozer/papers/timeseries.html">http://www.cs.colorado.edu/~mozer/papers/timeseries.html.</a>
<dt><dd><p>
   Weigend, A.S. and Gershenfeld, N.A., eds. (1994) <cite>Time Series
   Prediction: Forecasting the Future and Understanding the Past</cite>,
   Reading, MA: Addison-Wesley.
<dt><dd><p>

</dl>




<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_inverse">How to learn an inverse of a function?
</a></H2>

Ordinarily, NNs learn a function <tt>Y = f(X)</tt>, where <tt>Y</tt> is
a vector of outputs, <tt>X</tt> is a vector of inputs, and <tt>f()</tt>
is the function to be learned. Sometimes, however, you may want to learn
an inverse of a function <tt>f(),</tt> that is, given <tt>Y,</tt> you
want to be able to find an <tt>X</tt> such that <tt>Y = f(X).</tt>
In general, there may be many different <tt>Xs</tt> that satisfy
the equation <tt>Y = f(X).</tt> <p>

For example, in robotics (DeMers and Kreutz-Delgado, 1996, 1997),
<tt>X</tt> might describe the positions of the joints in a robot's arm,
while <tt>Y</tt> would describe the location of the robot's hand.  There
are simple formulas to compute the location of the hand given the
positions of the joints, called the "forward kinematics" problem.  But
there is no simple formula for the "inverse kinematics" problem to
compute positions of the joints that yield a given location for the
hand. Furthermore, if the arm has several joints, there will usually be
many different positions of the joints that yield the same location of
the hand, so the forward kinematics function is many-to-one and has no
unique inverse. Picking any <tt>X</tt> such that <tt>Y = f(X)</tt> is OK
if the only aim is to position the hand at <tt>Y.</tt> However if the
aim is to generate a series of points to move the hand through an arc
this may be insufficient. In this case the series of <tt>Xs</tt> need to
be in the same "branch" of the function space.  Care must be taken to
avoid solutions that yield inefficient or impossible movements of the
arm. <p>

As another example, consider an industrial process in which <tt>X</tt>
represents settings of control variables imposed by an operator, and
<tt>Y</tt> represents measurements of the product of the industrial
process. The function <tt>Y = f(X)</tt> can be learned by a NN using
conventional training methods. But the goal of the analysis may be to
find control settings <tt>X</tt> that yield a product with specified
measurements <tt>Y,</tt> in which case an inverse of <tt>f(X)</tt> is
required. In industrial applications, financial considerations are
important, so not just any setting <tt>X</tt> that yields the desired
result <tt>Y</tt> may be acceptable. Perhaps a function can be specified
that gives the cost of <tt>X</tt> resulting from energy consumption, raw
materials, etc., in which case you would want to find the <tt>X</tt>
that minimizes the cost function while satisfying the equation
<tt>Y = f(X).</tt> <p>

The obvious way to try to learn an inverse function is to generate a set
of training data from a given forward function, but designate <tt>Y</tt>
as the input and <tt>X</tt> as the output when training the network.
Using a least-squares error function, this approach will fail if
<tt>f()</tt> is many-to-one. The problem is that for an input
<tt>Y,</tt> the net will not learn any single <tt>X</tt> such that
<tt>Y = f(X),</tt> but will instead learn the arithmetic mean of all the
<tt>Xs</tt> in the training set that satisfy the equation (Bishop, 1995,
pp. 207-208). One solution to this difficulty is to construct a network
that learns a mixture approximation to the conditional distribution of
<tt>X</tt> given <tt>Y</tt> (Bishop, 1995, pp. 212-221).  However, the
mixture method will not work well in general for an <tt>X</tt> vector
that is more than one-dimensional, such as <tt>Y = X_1^2 + X_2^2,</tt>
since the number of mixture components required may increase
exponentially with the dimensionality of <tt>X.</tt> And you are still
left with the problem of extracting a single output vector from the
mixture distribution, which is nontrivial if the mixture components
overlap considerably.  Another solution is to use a highly robust error
function, such as a redescending M-estimator, that learns a single mode
of the conditional distribution instead of learning the mean (Huber,
1981; Rohwer and van der Rest 1996). Additional regularization terms or
constraints may be required to persuade the network to choose
appropriately among several modes, and there may be severe problems with
local optima. <p>

Another approach is to train a network to learn the forward mapping
<tt>f()</tt> and then numerically invert the function. Finding
<tt>X</tt> such that <tt>Y = f(X)</tt> is simply a matter of solving a
nonlinear system of equations, for which many algorithms can be found in
the numerical analysis literature (Dennis and Schnabel 1983).  One way
to solve nonlinear equations is turn the problem into an optimization
problem by minimizing <tt>sum(Y_i-f(X_i))^2.</tt> This method fits in
nicely with the usual gradient-descent methods for training NNs
(Kindermann and Linden 1990).  Since the nonlinear equations will
generally have multiple solutions, there may be severe problems with
local optima, especially if some solutions are considered more desirable
than others.  You can deal with multiple solutions by inventing some
objective function that measures the goodness of different solutions,
and optimizing this objective function under the nonlinear constraint
<tt>Y = f(X)</tt> using any of numerous algorithms for nonlinear
programming (NLP; see Bertsekas, 1995, and other references under

<a href="FAQ2.html#A_numanal">"What are conjugate gradients, Levenberg-Marquardt, etc.?")</a>
The power and flexibility of the nonlinear programming approach are
offset by possibly high computational demands. <p>

If the forward mapping <tt>f()</tt> is obtained by training a network,
there will generally be some error in the network's outputs. The
magnitude of this error can be difficult to estimate. The process of
inverting a network can propagate this error, so the results should be
checked carefully for validity and numerical stability. Some training
methods can produce not just a point output but also a prediction
interval (Bishop, 1995; White, 1992). You can take advantage of
prediction intervals when inverting a network by using NLP methods. For
example, you could try to find an <tt>X</tt> that minimizes the width of
the prediction interval under the constraint that the equation
<tt>Y = f(X)</tt> is satisfied. Or instead of requiring
<tt>Y = f(X)</tt> be satisfied exactly, you could try to find an
<tt>X</tt> such that the prediction interval is contained within some
specified interval while minimizing some cost function. <p>

For more mathematics concerning the inverse-function problem, as well as
some interesting methods involving self-organizing maps, see DeMers and
Kreutz-Delgado (1996, 1997). For NNs that are relatively easy to invert,
see the <a href="FAQ5.html#ALN">Adaptive Logic Networks</a> described in
the software sections of the FAQ.  <p>

References:
<dl>
<dt><dd><p>
   Bertsekas, D. P. (1995), <cite>Nonlinear Programming,</cite>
   Belmont, MA: Athena Scientific.
<dt><dd><p>
   Bishop, C.M. (1995), <cite>Neural Networks for Pattern Recognition</cite>,
   Oxford: Oxford University Press. 
<dt><dd><p>
   DeMers, D., and Kreutz-Delgado, K. (1996), "Canonical Parameterization
   of Excess motor degrees of freedom with self organizing maps",
   IEEE Trans Neural Networks, 7, 43-55.
<dt><dd><p>
   DeMers, D., and Kreutz-Delgado, K. (1997), "Inverse kinematics of
   dextrous manipulators," in Omidvar, O., and van der Smagt, P., (eds.)
   <cite>Neural Systems for Robotics,</cite> San Diego: Academic Press,
   pp. 75-116.
<dt><dd><p>
   Dennis, J.E. and Schnabel, R.B. (1983) <cite>Numerical Methods for
   Unconstrained Optimization and Nonlinear Equations,</cite> Prentice-Hall
<dt><dd><p>
   Huber, P.J. (1981), <cite>Robust Statistics,</cite> NY: Wiley.
<dt><dd><p>
   Kindermann, J., and Linden, A. (1990), "Inversion of Neural Networks
   by Gradient Descent," Parallel Computing, 14, 277-286,
   ftp://icsi.Berkeley.EDU/pub/ai/linden/KindermannLinden.IEEE92.ps.Z
<dt><dd><p>
   Rohwer, R., and van der Rest, J.C. (1996), "Minimum description length,
   regularization, and multimodal data," Neural Computation, 8, 595-609.
<dt><dd><p>
   White, H. (1992), "Nonparametric Estimation of Conditional Quantiles
   Using Neural Networks," in Page, C. and Le Page, R. (eds.), 
   <cite>Proceedings of the 23rd Sympsium on the Interface: Computing
   Science and Statistics,</cite> Alexandria, VA: American Statistical
   Association, pp. 190-199.
</dl>


<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_invariant">How to get invariant recognition of
    images under translation, rotation, etc.?</a></H2>

See:
<dl>
<dt><dd><p>
   Bishop, C.M. (1995), <cite>Neural Networks for Pattern Recognition</cite>,
   Oxford: Oxford University Press, section 8.7. 
<dt><dd><p>
   Masters, T. (1994), <cite>Signal and Image Processing with Neural
   Networks: A C++ Sourcebook</cite>, NY: Wiley.
<dt><dd><p>
   Soucek, B., and The IRIS Group (1992), <cite>Fast Learning and
   Invariant Object Recognition,</cite> NY: Wiley.
<dt><dd><p>
   Squire, D. (1997), <cite>Model-Based Neural Networks for Invariant
   Pattern Recognition,</cite>
   <a href="http://cuiwww.unige.ch/~squire/publications.html">http://cuiwww.unige.ch/~squire/publications.html</a>
<dt><dd><p>
   Laurenz Wiskott, bibliography on "Unsupervised Learning of Invariances in Neural Systems"
   <a href="http://www.cnl.salk.edu/~wiskott/Bibliographies/LearningInvariances.html">http://www.cnl.salk.edu/~wiskott/Bibliographies/LearningInvariances.html</a>
</dl>


<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_handwrite">How to recognize handwritten
    characters?</a></H2>

URLS:<ul>
<li>Don Tveter's <cite>The Pattern Recognition Basis of AI</cite> at
<a href="http://www.dontveter.com/basisofai/char.html">http://www.dontveter.com/basisofai/char.html</a>
<li>Andras Kornai's homepage at 
<a href="http://www.cs.rice.edu/~andras/">http://www.cs.rice.edu/~andras/</a>
<li>Yann LeCun's homepage at 
<a href="http://www.research.att.com/~yann/">http://www.research.att.com/~yann/</a><br>
Data sets of handwritten digits can be found at
<a href="http://www.research.att.com/~yann/exdb/mnist/">http://www.research.att.com/~yann/exdb/mnist/</a>
</ul>
<p>

Other references:
<dl>
<dt><dd><p>
    Hastie, T., and Simard, P.Y. (1998), "Metrics and models for
    handwritten character recognition," Statistical Science, 13, 54-65.
<dt><dd><p>
    Jackel, L.D. et al., (1994) "Comparison of Classifier Methods:
    A Case Study in Handwritten Digit Recognition", 
    1994 International Conference on Pattern Recognition, Jerusalem 
<dt><dd><p>
    LeCun, Y., Jackel, L.D., Bottou, L., Brunot, A.,
    Cortes, C., Denker, J.S., Drucker, H., Guyon, I.,
    Muller, U.A., Sackinger, E., Simard, P., and Vapnik, V. (1995),
    "Comparison of learning algorithms for handwritten digit
    recognition," in F. Fogelman and P. Gallinari, eds.,
    International Conference on Artificial Neural Networks,
    pages 53-60, Paris.
<dt><dd><p>
    Orr, G.B., and Mueller, K.-R., eds. (1998), <cite>Neural Networks:
    Tricks of the Trade,</cite> Berlin: Springer, ISBN 3-540-65311-2.
</dl>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_spike">What about pulsed or spiking NNs?</a></H2>

The standard reference is:

<dl>
<dt><dd><p>
   Maass, W., and Bishop, C.M., eds. (1999)
   <cite>Pulsed Neural Networks,</cite> 
   Cambridge, MA: The MIT Press, ISBN: 0262133504.
</dl>

For more information on this book, see the section on
<a href="FAQ4.html#A_books_notable_pulsed">"Pulsed/Spiking networks"</a>
under <a href="#A_books_notable">"Other notable books"</a>
in part 4 of the FAQ.
Also see Professor Maass's web page at
<a href="http://www.igi.tugraz.at/maass/">http://www.igi.tugraz.at/maass/.</a><p>

Some other interesting URLs include:
<ul>
<li> Laboratory of Computational Neuroscience (LCN)
     at the Swiss Federal Institute of Technology Lausanne, 
     <a href="http://diwww.epfl.ch/mantra/mantra_bioneuro.html">http://diwww.epfl.ch/mantra/mantra_bioneuro.html</a>
     <p>
<li> The notoriously hyped Berger-Liaw Neural Network
     Speaker-Independent Speech Recognition System, 
     <a href="http://www.usc.edu/ext-relations/news_service/releases/stories/36013.html">http://www.usc.edu/ext-relations/news_service/releases/stories/36013.html</a>
</ul>



<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_genetic">What about Genetic Algorithms?</a></H2>

There are a number of definitions of GA (Genetic Algorithm).
A possible one is<p>
<pre>
  A GA is an optimization program
  that starts with
  a population of encoded procedures,       (Creation of Life :-> )
  mutates them stochastically,              (Get cancer or so :-> )
  and uses a selection process              (Darwinism)
  to prefer the mutants with high fitness
  and perhaps a recombination process       (Make babies :-> )
  to combine properties of (preferably) the succesful mutants.
</pre>

Genetic algorithms are just a special case of the more general idea
of "evolutionary computation".
There is a newsgroup that is dedicated to the field of evolutionary
computation called comp.ai.genetic.
It has a detailed FAQ posting which, for instance, explains the terms
"Genetic Algorithm", "Evolutionary Programming", "Evolution Strategy",
"Classifier System", and "Genetic Programming".
That FAQ also contains lots of pointers to relevant literature, software,
other sources of information, et cetera et cetera.
Please see the comp.ai.genetic FAQ for further information. <p>

For an entertaining introduction to evolutionary training of neural
nets, see:
<dl>
<dt><dd><p>
   David Fogel (2001), <cite>Blondie24: Playing at the Edge of AI,</cite>
   Morgan Kaufmann Publishers, ISBN: 1558607838
</dl>
There are other books and papers by Fogel and his colleagues listed under
<a href="FAQ7.html#A_app_games_checkers">"Checkers/Draughts"</a> in the
<a href="FAQ7.html#A_app_games">"Games, sports, gambling"</a>
section above. <p>

For an extensive review, see:
<dl>
<dt><dd><p>
   Yao, X. (1999), "Evolving Artificial Neural Networks,"
   Proceedings of the IEEE, 87, 1423-1447,
   <a href="http://www.cs.bham.ac.uk/~xin/journal_papers.html">http://www.cs.bham.ac.uk/~xin/journal_papers.html</a>
</dl>
<p>

Here are some other on-line papers about evolutionary training of NNs:
<ul>
<p>
<li>Backprop+GA: <a href="http://geneura.ugr.es/~pedro/G-Prop.htm">http://geneura.ugr.es/~pedro/G-Prop.htm</a>
<p>
<li>LVQ+GA: <a href="http://geneura.ugr.es/g-lvq/g-lvq.html">http://geneura.ugr.es/g-lvq/g-lvq.html</a>
<p>
<li>Very long chromosomes: <a href="ftp://archive.cis.ohio-state.edu/pub/neuroprose/korning.nnga.ps.Z">ftp://archive.cis.ohio-state.edu/pub/neuroprose/korning.nnga.ps.Z</a>
<p>
</ul>
<p>

More URLs on genetic algorithms and NNs:
<ul>
<li>Omri Weisman and Ziv Pollack's web page on
"Neural Network Using Genetic Algorithms" at
<a href="http://www.cs.bgu.ac.il/~omri/NNUGA/">http://www.cs.bgu.ac.il/~omri/NNUGA/</a>
<p>
<li>Christoph M. Friedrich's web page on 
Evolutionary algorithms and Artificial Neural Networks has a bibloigraphy
and links to researchers at
<a href="http://www.tussy.uni-wh.de/~chris/gann/gann.html">http://www.tussy.uni-wh.de/~chris/gann/gann.html</a>
<p>
<li>Andrew Gray's Hybrid Systems FAQ at the University of Otago at
<a href="http://divcom.otago.ac.nz:800/COM/INFOSCI/SMRL/people/andrew/publications/faq/hybrid/hybrid.htm">
http://divcom.otago.ac.nz:800/COM/INFOSCI/SMRL/people/andrew/publications/faq/hybrid/hybrid.htm</a>
<p>
<li>Differential Evolution: <a href="http://www.icsi.berkeley.edu/~storn/code.html">http://www.icsi.berkeley.edu/~storn/code.html</a>
<p>
</ul>

For general information on GAs, try the links at 
<a href="http://www.shef.ac.uk/~gaipp/galinks.html">http://www.shef.ac.uk/~gaipp/galinks.html</a>
and
<a href="http://www.cs.unibo.it/~gaioni">http://www.cs.unibo.it/~gaioni</a>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_fuzzy">What about Fuzzy Logic?</a></H2>

Fuzzy logic is an area of research based on the work of L.A. Zadeh.
It is a departure from classical two-valued sets and logic, that uses
"soft" linguistic (e.g. large, hot, tall) system variables and a
continuous range of truth values in the interval [0,1], rather than
strict binary (True or False) decisions and assignments.<p>

Fuzzy logic is used where a system is difficult to model exactly (but
an inexact model is available), is controlled by a human operator or
expert, or where ambiguity or vagueness is common.  A typical fuzzy
system consists of a rule base, membership functions, and an inference
procedure.<p>

Most fuzzy logic discussion takes place in the newsgroup comp.ai.fuzzy
(where there is a fuzzy logic FAQ) but there is also some work (and
discussion) about combining fuzzy logic with neural network approaches
in comp.ai.neural-nets.<p>

Early work combining neural nets and fuzzy methods used competitive
networks to generate rules for fuzzy systems (Kosko 1992). This approach
is sort of a crude version of bidirectional counterpropagation
(Hecht-Nielsen 1990) and suffers from the same deficiencies. More recent
work (Brown and Harris 1994; Kosko 1997) has been based on the realization that a
fuzzy system is a nonlinear mapping from an input space to an output
space that can be parameterized in various ways and therefore can be
adapted to data using the usual neural training methods (see <a
href="FAQ2.html#A_backprop">"What is backprop?")</a> or conventional numerical
optimization algorithms (see <a href="FAQ2.html#A_numanal">"What are
conjugate gradients, Levenberg-Marquardt, etc.?").</a> <p>

A neural net can incorporate fuzziness in various ways:
<ul>
<li>The inputs can be fuzzy. Any garden-variety backprop net is
fuzzy in this sense, and it seems rather silly to call a net "fuzzy"
solely on this basis, although Fuzzy ART (Carpenter and Grossberg 1996) 
has no other fuzzy characteristics.

<li>The outputs can be fuzzy. Again, any garden-variety backprop net is
fuzzy in this sense. But competitive learning nets ordinarily produce
crisp outputs, so for competitive learning methods, having fuzzy output
is a meaningful distinction. For example, fuzzy c-means clustering
(Bezdek 1981) is meaningfully different from (crisp) k-means.
Fuzzy ART does <em>not</em> have fuzzy outputs.

<li>The net can be interpretable as an adaptive fuzzy system.  For
example, Gaussian RBF nets and B-spline regression models (Dierckx 1995,
van Rijckevorsal 1988) are fuzzy systems with adaptive weights (Brown
and Harris 1994) and can legitimately be called neurofuzzy systems.

<li>The net can be a conventional NN architecture that operates on
fuzzy numbers instead of real numbers (Lippe, Feuring and Mischke 1995).

<li>Fuzzy constraints can provide external knowledge (Lampinen and 
Selonen 1996).

</ul>

More information on neurofuzzy systems is available online:
<ul>
<li>The Fuzzy Logic and Neurofuzzy Resources page of the
Image, Speech and Intelligent Systems (ISIS) research group at the
University of Southampton, Southampton, Hampshire, UK:  
<a href="http://www-isis.ecs.soton.ac.uk/research/nfinfo/fuzzy.html">
http://www-isis.ecs.soton.ac.uk/research/nfinfo/fuzzy.html</a>.

<li> The Neuro-Fuzzy Systems Research Group's web page at 
Tampere University of Technology, Tampere, Finland:
<a href="http://www.cs.tut.fi/~tpo/group.html">http://www.cs.tut.fi/~tpo/group.html</a>
and <a href="http://dmiwww.cs.tut.fi/nfs/Welcome_uk.html">http://dmiwww.cs.tut.fi/nfs/Welcome_uk.html</a>

<li> Marcello Chiaberge's Neuro-Fuzzy page 
at <a href="http://polimage.polito.it/~marcello">http://polimage.polito.it/~marcello</a>.
<! marcello@polimage.polito.it> 

<li>The homepage of the research group on Neural Networks and Fuzzy Systems at the Institute of
Knowledge Processing and Language Engineering, Faculty of Computer Science, University of Magdeburg, Germany, at
<a href="http://www.neuro-fuzzy.de/">http://www.neuro-fuzzy.de/</a>

<li> Jyh-Shing Roger Jang's home page at 
<a href="http://www.cs.nthu.edu.tw/~jang/">
http://www.cs.nthu.edu.tw/~jang/</a>
with information on ANFIS (Adaptive Neuro-Fuzzy Inference Systems),
articles on neuro-fuzzy systems, and more links.

<li>Andrew Gray's Hybrid Systems FAQ at the University of Otago at
<a href="http://divcom.otago.ac.nz:800/COM/INFOSCI/SMRL/people/andrew/publications/faq/hybrid/hybrid.htm">
http://divcom.otago.ac.nz:800/COM/INFOSCI/SMRL/people/andrew/publications/faq/hybrid/hybrid.htm</a>

</ul>

References:
<dl>
<dt><dd><p>
   Bezdek, J.C. (1981), <cite>Pattern Recognition with Fuzzy Objective
   Function Algorithms,</cite> New York: Plenum Press.
<dt><dd><p>
   Bezdek, J.C. & Pal, S.K., eds. (1992), <cite>Fuzzy Models for
   Pattern Recognition,</cite> New York: IEEE Press.
<dt><dd><p>
   Brown, M., and Harris, C. (1994), <cite>Neurofuzzy Adaptive 
   Modelling and Control,</cite> NY: Prentice Hall.
<dt><dd><p>
   Carpenter, G.A. and Grossberg, S. (1996), "Learning, Categorization,
   Rule Formation, and Prediction by Fuzzy Neural Networks," in
   Chen, C.H. (1996), pp. 1.3-1.45.
<dt><dd><p>
   Chen, C.H., ed. (1996) <cite>Fuzzy Logic and Neural Network Handbook,</cite>
   NY: McGraw-Hill, ISBN 0-07-011189-8.
<dt><dd><p>
   Dierckx, P. (1995), <cite>Curve and Surface Fitting with Splines,</cite>
   Oxford: Clarendon Press.
<dt><dd><p>
   Hecht-Nielsen, R. (1990), <cite>Neurocomputing</cite>, Reading, MA:
   Addison-Wesley. 
<dt><dd><p>
   Klir, G.J. and Folger, T.A.(1988), <cite>Fuzzy Sets, Uncertainty, and
   Information,</cite> Englewood Cliffs, N.J.: Prentice-Hall.
<dt><dd><p>
   Kosko, B.(1992), <cite>Neural Networks and Fuzzy Systems,</cite> 
   Englewood Cliffs, N.J.: Prentice-Hall.
<dt><dd><p>
   Kosko, B. (1997), <cite>Fuzzy Engineering,</cite> NY: Prentice Hall.
<dt><dd><p>
   Lampinen, J and Selonen, A. (1996), "Using Background Knowledge for
   Regularization of Multilayer Perceptron Learning", Submitted to
   International Conference on Artificial Neural Networks, ICANN'96,
   Bochum, Germany.
<dt><dd><p>
   Lippe, W.-M., Feuring, Th. and Mischke, L. (1995),
   "Supervised learning in fuzzy neural networks,"
   Institutsbericht Angewandte Mathematik und Informatik, WWU Muenster,
   I-12,
   <A HREF="http://wwwmath.uni-muenster.de/~feuring/WWW_literatur/bericht12_95.ps.gz">
   http://wwwmath.uni-muenster.de/~feuring/WWW_literatur/bericht12_95.ps.gz</a>
<dt><dd><p>
   Nauck, D., Klawonn, F., and Kruse, R. (1997),
   <cite>Foundations of Neuro-Fuzzy Systems,</cite>
   Chichester: Wiley, ISBN 0-471-97151-0.
<dt><dd><p>
   van Rijckevorsal, J.L.A. (1988), "Fuzzy coding and B-splines," in
   van Rijckevorsal, J.L.A., and de Leeuw, J., eds., Component and
   Correspondence Analysis, Chichester: John Wiley & Sons, pp. 33-54.
</dl>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_un">Unanswered FAQs</a></H2>

<ul>
<li>How many training cases do I need?
<li>How should I split the data into training and validation sets?
<li>What error functions can be used?
<li>How can I select important input variables?
<li>Should NNs be used in safety-critical applications?
</ul>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_links">Other NN links?</a></H2>

<ul>

<H3><li>Search engines</H3>

<ul>

<li>Yahoo:
    <a href="http://www.yahoo.com/Science/Engineering/Electrical_Engineering/Neural_Networks/">
    http://www.yahoo.com/Science/Engineering/Electrical_Engineering/Neural_Networks/</a>

<li>Neuroscience Web Search:
    <a href="http://www.acsiom.org/nsr/neuro.html">
             http://www.acsiom.org/nsr/neuro.html</a>

</ul>

<H3><li>Archives of NN articles and software</H3>

<ul>

<H4><li>Neuroprose ftp archive site</H4>

   <a href="ftp://archive.cis.ohio-state.edu/pub/neuroprose">
            ftp://archive.cis.ohio-state.edu/pub/neuroprose</a>

   This directory contains technical reports as a public service to the
   connectionist and neural network scientific community.

<H4><li>Finnish University Network archive site</H4>
   A large collection of neural network papers and
   software at
   
   <a href="ftp://ftp.funet.fi/pub/sci/neural/">
            ftp://ftp.funet.fi/pub/sci/neural/</a>

   Contains all the public domain software and papers that they
   have been able to find.
   All of these files have been transferred from FTP sites in U.S.
   and are mirrored about every 3 months at fastest.
   Contact: neural-adm@ftp.funet.fi

<H4><li>SEL-HPC Article Archive</H4>
    <a href="http://liinwww.ira.uka.de/bibliography/Misc/SEL-HPC.html">
    http://liinwww.ira.uka.de/bibliography/Misc/SEL-HPC.html</a>

<H4><li>Machine Learning Papers</H4>

   <a href="http://gubbio.cs.berkeley.edu/mlpapers/">
   http://gubbio.cs.berkeley.edu/mlpapers/</a>

</ul>

<H3><li>Plain-text Tables of Contents of NN journals</H3>
   Pattern Recognition Group, Department of Applied Physics,<br>
   Faculty of Applied Sciences, Delft University of Technology,<br>
<a href="http://www.ph.tn.tudelft.nl/PRInfo/PRInfo/journals.html">http://www.ph.tn.tudelft.nl/PRInfo/PRInfo/journals.html</a>

<H3><li>The Collection of Computer Science Bibliographies: Bibliographies on Neural Networks</H4>
    <a href="http://liinwww.ira.uka.de/bibliography/Neural/index.html">
    http://liinwww.ira.uka.de/bibliography/Neural/index.html</a>

<H3><li>BibTeX data bases of NN journals</H3>
   The Center for Computational Intelligence maintains BibTeX data bases
   of various NN journals, including IEEE Transactions on Neural
   Networks, Machine Learning, Neural Computation, and NIPS, at
<a href="http://www.ci.tuwien.ac.at/docs/ci/bibtex_collection.html">
   http://www.ci.tuwien.ac.at/docs/ci/bibtex_collection.html</a>
or
<a href="ftp://ftp.ci.tuwien.ac.at/pub/texmf/bibtex/bib/">
   ftp://ftp.ci.tuwien.ac.at/pub/texmf/bibtex/bib/</a>.

<H3><li>NN events server</H3>
   There is a WWW page for Announcements of Conferences,
   Workshops and Other Events on Neural Networks at IDIAP in Switzerland.
   WWW-Server: <a href="http://www.idiap.ch/html/idiap-networks.html">
                        http://www.idiap.ch/html/idiap-networks.html</a>.

<H3><li>Academic programs list</H3>
   Rutvik Desai &lt;rutvik@c3serve.c3.lanl.gov&gt;
   has a compilation of acedemic programs offering
   interdeciplinary studies in computational neuroscience, AI,
   cognitive psychology etc. at
   <A HREF="http://www.cs.indiana.edu/hyplan/rudesai/cogsci-prog.html">
   http://www.cs.indiana.edu/hyplan/rudesai/cogsci-prog.html</A> <P>
   Links to neurosci, psychology, linguistics lists are also
   provided.

<H3><li>Neurosciences Internet Resource Guide</H3>
   This document aims to be a guide to existing, free, Internet-accessible
   resources helpful to neuroscientists of all stripes.
   An ASCII text version (86K) is available in the
   Clearinghouse of Subject-Oriented Internet Resource Guides as
   follows:<p>
   <a href="ftp://una.hh.lib.umich.edu/inetdirsstacks/neurosci:cormbonario">
     ftp://una.hh.lib.umich.edu/inetdirsstacks/neurosci:cormbonario</a>,
   <a href="gopher://una.hh.lib.umich.edu/00/inetdirsstacks/neurosci:cormbonario">
     gopher://una.hh.lib.umich.edu/00/inetdirsstacks/neurosci:cormbonario</a>,
   <a href="http://http2.sils.umich.edu/Public/nirg/nirg1.html">
     http://http2.sils.umich.edu/Public/nirg/nirg1.html</a>.

<H3><li>Other WWW sites</H3>
   In World-Wide-Web (WWW, for example via the xmosaic program) you
   can read neural network information for instance by opening one
   of the following uniform resource locators (URLs): <br>
   <a href="http://www-xdiv.lanl.gov/XCM/neural/neural_announcements.html">
            http://www-xdiv.lanl.gov/XCM/neural/neural_announcements.html</a>
            Los Alamos neural announcements and general information, <br>
   <a href="http://www.ph.kcl.ac.uk/neuronet/">
     http://www.ph.kcl.ac.uk/neuronet/</a> (NEuroNet, King's College, London), <br>
   <a href="http://www.eeb.ele.tue.nl">
     http://www.eeb.ele.tue.nl</a> (Eindhoven, Netherlands), <br>
   <a href="http://www.emsl.pnl.gov:2080/docs/cie/neural/">
     http://www.emsl.pnl.gov:2080/docs/cie/neural/</a>
     (Pacific Northwest National Laboratory, Richland, Washington, USA), <br>
   <a href="http://www.cosy.sbg.ac.at/~rschwaig/rschwaig/projects.html">
     http://www.cosy.sbg.ac.at/~rschwaig/rschwaig/projects.html</a>
     (Salzburg, Austria), <br>
   <a href="http://http2.sils.umich.edu/Public/nirg/nirg1.html">
     http://http2.sils.umich.edu/Public/nirg/nirg1.html</a>
     (Michigan, USA), <br>
   <a href="http://www.lpac.ac.uk/SEL-HPC/Articles/NeuralArchive.html">
     http://www.lpac.ac.uk/SEL-HPC/Articles/NeuralArchive.html</a>
     (London), <br>
   <a href="http://rtm.science.unitn.it/">
     http://rtm.science.unitn.it/</a>
     Reactive Memory Search (Tabu Search) page (Trento, Italy), <br>
   <a href="http://www.wi.leidenuniv.nl/art/">
     http://www.wi.leidenuniv.nl/art/</a> (ART WWW site, Leiden, Netherlands), <br>
   <a href="http://nucleus.hut.fi/nnrc/">
     http://nucleus.hut.fi/nnrc/</a> Helsinki University of Technology. <br>
   <a href="http://www.pitt.edu/~mattf/NeuroRing.html">
     http://www.pitt.edu/~mattf/NeuroRing.html</a> links to neuroscience web pages <br>
   <a href="http://www.arcade.uiowa.edu/hardin-www/md-neuro.html">
     http://www.arcade.uiowa.edu/hardin-www/md-neuro.html</a>Hardin Meta Directory web
     page for Neurology/Neurosciences.<br>
   Many others are available too; WWW is changing all the time.

</ul>


<pre>
------------------------------------------------------------------------
</pre>

That's all folks (End of the Neural Network FAQ).<p>

<pre>
Acknowledgements: Thanks to all the people who helped to get the stuff
                  above into the posting. I cannot name them all, because
                  I would make far too many errors then. :->

                  No?  Not good?  You want individual credit?
                  OK, OK. I'll try to name them all. But: no guarantee....

  THANKS FOR HELP TO:
(in alphabetical order of email adresses, I hope)
</pre>
<ul>
<li>Steve Ward &lt;71561.2370@CompuServe.COM&gt;
<li>Allen Bonde &lt;ab04@harvey.gte.com&gt;
<li>Accel Infotech Spore Pte Ltd &lt;accel@solomon.technet.sg&gt;
<li> Ales Krajnc &lt;akrajnc@fagg.uni-lj.si&gt;
<li>Alexander Linden &lt;al@jargon.gmd.de&gt;
<li>Matthew David Aldous &lt;aldous@mundil.cs.mu.OZ.AU&gt;
<li>S.Taimi Ames &lt;ames@reed.edu&gt;
<li>Axel Mulder &lt;amulder@move.kines.sfu.ca&gt;
<li>anderson@atc.boeing.com
<li>Andy Gillanders &lt;andy@grace.demon.co.uk&gt;
<li>Davide Anguita &lt;anguita@ICSI.Berkeley.EDU&gt;
<li>Avraam Pouliakis &lt;apou@leon.nrcps.ariadne-t.gr&gt;
<li>Kim L. Blackwell &lt;avrama@helix.nih.gov&gt;
<li>Mohammad Bahrami &lt;bahrami@cse.unsw.edu.au&gt;
<li>Paul Bakker &lt;bakker@cs.uq.oz.au&gt;
<li>Stefan Bergdoll &lt;bergdoll@zxd.basf-ag.de&gt;
<li>Jamshed Bharucha &lt;bharucha@casbs.Stanford.EDU&gt;
<li>Carl M. Cook &lt;biocomp@biocomp.seanet.com&gt;
<li>Yijun Cai &lt;caiy@mercury.cs.uregina.ca&gt;
<li>L. Leon Campbell &lt;campbell@brahms.udel.edu&gt;
<li>Cindy Hitchcock &lt;cindyh@vnet.ibm.com&gt;
<li>Clare G. Gallagher &lt;clare@mikuni2.mikuni.com&gt;
<li>Craig Watson &lt;craig@magi.ncsl.nist.gov&gt
<li>Yaron Danon &lt;danony@goya.its.rpi.edu&gt;
<li>David Ewing &lt;dave@ndx.com&gt;
<li>David DeMers &lt;demers@cs.ucsd.edu&gt;
<li>Denni Rognvaldsson &lt;denni@thep.lu.se&gt;
<li>Duane Highley &lt;dhighley@ozarks.sgcl.lib.mo.us&gt;
<li>Dick.Keene@Central.Sun.COM
<li>DJ Meyer &lt;djm@partek.com&gt;
<li>Donald Tveter &lt;don@dontveter.com&gt;
<li>Daniel Tauritz &lt;dtauritz@wi.leidenuniv.nl&gt;
<li>Wlodzislaw Duch &lt;duch@phys.uni.torun.pl&gt;
<li>E. Robert Tisdale &lt;edwin@flamingo.cs.ucla.edu&gt;
<li>Athanasios Episcopos &lt;episcopo@fire.camp.clarkson.edu&gt;
<li>Frank Schnorrenberg &lt;fs0997@easttexas.tamu.edu&gt;
<li>Gary Lawrence Murphy &lt;garym@maya.isis.org&gt;
<li>gaudiano@park.bu.edu
<li>Lee Giles &lt;giles@research.nj.nec.com&gt;
<li>Glen Clark &lt;opto!glen@gatech.edu&gt;
<li>Phil Goodman &lt;goodman@unr.edu&gt;
<li>guy@minster.york.ac.uk
<li>Horace A. Vallas, Jr. &lt;hav@neosoft.com&gt;
<li>Joerg Heitkoetter &lt;heitkoet@lusty.informatik.uni-dortmund.de&gt;
<li>Ralf Hohenstein &lt;hohenst@math.uni-muenster.de&gt;
<li>Ian Cresswell &lt;icressw@leopold.win-uk.net&gt;
<li>Gamze Erten &lt;ictech@mcimail.com&gt;
<li>Ed Rosenfeld &lt;IER@aol.com&gt;
<li>Franco Insana &lt;INSANA@asri.edu&gt;
<li>Janne Sinkkonen &lt;janne@iki.fi&gt;
<li>Javier Blasco-Alberto &lt;jblasco@ideafix.cps.unizar.es&gt;
<li>Jean-Denis Muller &lt;jdmuller@vnet.ibm.com&gt;
<li>Jeff Harpster &lt;uu0979!jeff@uu9.psi.com&gt;
<li>Jonathan Kamens &lt;jik@MIT.Edu&gt;
<li>J.J. Merelo &lt;jmerelo@geneura.ugr.es&gt;
<li>Dr. Jacek Zurada &lt;jmzura02@starbase.spd.louisville.edu&gt;
<li>Jon Gunnar Solheim &lt;jon@kongle.idt.unit.no&gt;
<li>Josef Nelissen &lt;jonas@beor.informatik.rwth-aachen.de&gt;
<li>Joey Rogers &lt;jrogers@buster.eng.ua.edu&gt;
<li>Subhash Kak &lt;kak@gate.ee.lsu.edu&gt;
<li>Ken Karnofsky &lt;karnofsky@mathworks.com&gt;
<li>Kjetil.Noervaag@idt.unit.no
<li>Luke Koops &lt;koops@gaul.csd.uwo.ca&gt;
<li>Kurt Hornik &lt;Kurt.Hornik@tuwien.ac.at&gt;
<li>Thomas Lindblad &lt;lindblad@kth.se&gt;
<li>Clark Lindsey &lt;lindsey@particle.kth.se&gt;
<li>Lloyd Lubet &lt;llubet@rt66.com&gt;
<li>William Mackeown &lt;mackeown@compsci.bristol.ac.uk&gt;
<li>Maria Dolores Soriano Lopez &lt;maria@vaire.imib.rwth-aachen.de&gt;
<li>Mark Plumbley &lt;mark@dcs.kcl.ac.uk&gt;
<li>Peter Marvit &lt;marvit@cattell.psych.upenn.edu&gt;
<li>masud@worldbank.org
<li>Miguel A. Carreira-Perpinan&lt;mcarreir@moises.ls.fi.upm.es&gt;
<li>Yoshiro Miyata &lt;miyata@sccs.chukyo-u.ac.jp&gt;
<li>Madhav Moganti &lt;mmogati@cs.umr.edu&gt;
<li>Jyrki Alakuijala &lt;more@ee.oulu.fi&gt;
<li>Jean-Denis Muller &lt;muller@bruyeres.cea.fr&gt;
<li>Michael Reiss &lt;m.reiss@kcl.ac.uk&gt;
<li>mrs@kithrup.com
<li>Maciek Sitnik &lt;msitnik@plearn.edu.pl&gt;
<li>R. Steven Rainwater &lt;ncc@ncc.jvnc.net&gt;
<li>Nigel Dodd &lt;nd@neural.win-uk.net&gt;
<li>Barry Dunmall &lt;neural@nts.sonnet.co.uk&gt;
<li>Paolo Ienne &lt;Paolo.Ienne@di.epfl.ch&gt;
<li>Paul Keller &lt;pe_keller@ccmail.pnl.gov&gt;
<li>Peter Hamer &lt;P.G.Hamer@nortel.co.uk&gt;
<li>Pierre v.d. Laar &lt;pierre@mbfys.kun.nl&gt;
<li>Michael Plonski &lt;plonski@aero.org&gt;
<li>Lutz Prechelt &lt;prechelt@ira.uka.de&gt; [creator of FAQ]
<li>Richard Andrew Miles Outerbridge &lt;ramo@uvphys.phys.uvic.ca&gt;
<li>Rand Dixon &lt;rdixon@passport.ca&gt;
<li>Robin L. Getz &lt;rgetz@esd.nsc.com&gt;
<li>Richard Cornelius &lt;richc@rsf.atd.ucar.edu&gt;
<li>Rob Cunningham &lt;rkc@xn.ll.mit.edu&gt;
<li>Robert.Kocjancic@IJS.si
<li>Randall C. O'Reilly &lt;ro2m@crab.psy.cmu.edu&gt;
<li>Rutvik Desai &lt;rudesai@cs.indiana.edu&gt;
<li>Robert W. Means &lt;rwmeans@hnc.com&gt;
<li>Stefan Vogt &lt;s_vogt@cis.umassd.edu&gt;
<li>Osamu Saito &lt;saito@nttica.ntt.jp&gt;
<li>Scott Fahlman &lt;sef+@cs.cmu.edu&gt;
<li>&lt;seibert@ll.mit.edu&gt;
<li>Sheryl Cormicle &lt;sherylc@umich.edu&gt;
<li>Ted Stockwell &lt;ted@aps1.spa.umn.edu&gt;
<li>Stephanie Warrick &lt;S.Warrick@cs.ucl.ac.uk&gt;
<li>Serge Waterschoot &lt;swater@minf.vub.ac.be&gt;
<li>Thomas G. Dietterich &lt;tgd@research.cs.orst.edu&gt;
<li>Thomas.Vogel@cl.cam.ac.uk
<li>Ulrich Wendl &lt;uli@unido.informatik.uni-dortmund.de&gt;
<li>M. Verleysen &lt;verleysen@dice.ucl.ac.be&gt;
<li>VestaServ@aol.com
<li>Sherif Hashem &lt;vg197@neutrino.pnl.gov&gt;
<li>Matthew P Wiener &lt;weemba@sagi.wistar.upenn.edu&gt;
<li>Wesley Elsberry &lt;welsberr@orca.tamu.edu&gt;
<li>Dr. Steve G. Romaniuk &lt;ZLXX69A@prodigy.com&gt;
</ul>
Special thanks to Gregory E. Heath &lt;heath@ll.mit.edu&gt; and
Will Dwinnell &lt;predictor@delphi.com&gt; for years of stimulating
and educational discussions on comp.ai.neurtal-nets. <p>

The FAQ was created in June/July 1991 by Lutz Prechelt;
he also maintained the FAQ until November 1995.
Warren Sarle maintains the FAQ since December 1995.
<pre>

Bye

  Warren & Lutz
</pre>

Previous part is <A HREF="FAQ6.html">part 6</A>. <P>

<address> Neural network FAQ / Warren S. Sarle, saswss@unx.sas.com</address>
