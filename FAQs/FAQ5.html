<pre>
Archive-name: ai-faq/neural-nets/part5
Last-modified: 2002-08-11
URL: ftp://ftp.sas.com/pub/neural/FAQ5.html
Maintainer: saswss@unx.sas.com (Warren S. Sarle)
</pre>
<title>Neural Network FAQ, part 5 of 7: Free Software</title>

The copyright for the description of each product is held by the
producer or distributor of the product or whoever it was who supplied
the description for the FAQ, who by submitting it for the FAQ gives
permission for the description to be reproduced as part of the FAQ
in any of the ways specified in part 1 of the FAQ. <p>

This is part 5 (of 7) of a monthly posting to the Usenet newsgroup
comp.ai.neural-nets. See the part 1 of this posting for full
information what it is all about.<p>

<H1><a name="questions">
========== Questions ==========
</a></H1>

<a href="FAQ.html#questions">
Part 1: Introduction</a><br>

<a href="FAQ2.html#questions">
Part 2: Learning</a><br>

<a href="FAQ3.html#questions">
Part 3: Generalization</a><br>

<a href="FAQ4.html#questions">
Part 4: Books, data, etc.</a><br>

Part 5: Free software<br>
<OL>
 <a href="FAQ5.html#A_source">Source code on the web?</a><br>
 <a href="FAQ5.html#A18">Freeware and shareware packages for NN simulation?</a><p>
</OL>

<a href="FAQ6.html#questions">
Part 6: Commercial software</a><br>

<a href="FAQ7.html#questions">
Part 7: Hardware and miscellaneous</a><br>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A_source">Source code on the web?</a></H2>

The following URLs are reputed to have source code for NNs.
Use at your own risk. <p>

<ul>
<li>C/C++<br>
<a href="http://www.generation5.org/xornet.shtml">http://www.generation5.org/xornet.shtml</a><br>
<a href="http://www.netwood.net/~edwin/Matrix/">http://www.netwood.net/~edwin/Matrix/</a><br>
<a href="http://www.netwood.net/~edwin/svmt/">http://www.netwood.net/~edwin/svmt/</a><br>
<a href="http://www.geocities.com/Athens/Agora/7256/c-plus-p.html">http://www.geocities.com/Athens/Agora/7256/c-plus-p.html</a><br>
<a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/faces.html">http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/faces.html</a><br>
<a href="http://www.cog.brown.edu/~rodrigo/neural_nets_library.html">http://www.cog.brown.edu/~rodrigo/neural_nets_library.html</a><br>
<a href="http://www.agt.net/public/bmarshal/aiparts/aiparts.htm">http://www.agt.net/public/bmarshal/aiparts/aiparts.htm</a><br>
<a href="http://www.geocities.com/CapeCanaveral/1624/">http://www.geocities.com/CapeCanaveral/1624/</a><br>
<a href="http://www.neuroquest.com/">http://www.neuroquest.com/</a> or
<a href="http://www.grobe.org/LANE">http://www.grobe.org/LANE</a><br>
<a href="http://www.neuro-fuzzy.de/">http://www.neuro-fuzzy.de/</a><br>
<a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/cascor/">http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/cascor/</a><br>
<a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/qprop/">http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/qprop/</a><br>
<a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/rcc/">http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/rcc/</a><br>
etc.
<a href=""></a><br>
<p>
<li>Java<br>
<a href="http://www.philbrierley.com/code">http://www.philbrierley.com/code</a><br>
<a href="http://rfhs8012.fh-regensburg.de/~saj39122/jfroehl/diplom/e-index.html">http://rfhs8012.fh-regensburg.de/~saj39122/jfroehl/diplom/e-index.html</a><br>
<a href="http://neuron.eng.wayne.edu/software.html">http://neuron.eng.wayne.edu/software.html</a><br>
<a href="http://www.aist.go.jp/NIBH/~b0616/Lab/Links.html">http://www.aist.go.jp/NIBH/~b0616/Lab/Links.html</a><br>
<a href="http://www.aist.go.jp/NIBH/~b0616/Lab/BSOM1/">http://www.aist.go.jp/NIBH/~b0616/Lab/BSOM1/</a><br>
<a href="http://www.neuroinformatik.ruhr-uni-bochum.de/ini/PEOPLE/loos">http://www.neuroinformatik.ruhr-uni-bochum.de/ini/PEOPLE/loos</a><br>
<a href="http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/research/gsn/DemoGNG/GNG.html">http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/research/gsn/DemoGNG/GNG.html</a><br>
<a href="http://www.isbiel.ch/I/Projects/janet/index.html">http://www.isbiel.ch/I/Projects/janet/index.html</a><br>
<a href="http://www.born-again.demon.nl/software.html">http://www.born-again.demon.nl/software.html</a><br>
<a href="http://www.patol.com/java/NN/index.html">http://www.patol.com/java/NN/index.html</a><br>
<a href="http://www-isis.ecs.soton.ac.uk/computing/neural/laboratory/laboratory.html">http://www-isis.ecs.soton.ac.uk/computing/neural/laboratory/laboratory.html</a><br>
<a href="http://www.neuro-fuzzy.de/">http://www.neuro-fuzzy.de/</a><br>
<a href="http://sourceforge.net/projects/joone">http://sourceforge.net/projects/joone</a><br>
<a href="http://www.geocities.com/aydingurel/neural/">http://www.geocities.com/aydingurel/neural/</a><br>
<a href="http://www-eco.enst-bretagne.fr/~phan/emergence/complexe/neuron/mlp.html">http://www-eco.enst-bretagne.fr/~phan/emergence/complexe/neuron/mlp.html</a><br>
<a href=""></a><br>
<p>
<li>FORTRAN<br>
<a href="http://www.philbrierley.com/code">http://www.philbrierley.com/code</a><br>
<a href="http://www.cranfield.ac.uk/public/me/fo941992/mlpcode.htm">http://www.cranfield.ac.uk/public/me/fo941992/mlpcode.htm</a><br>
<a href=""></a><br>
<p>
<li>Pascal<br>
<a href="http://www.ibrtses.com/delphi/neuralnets.html">http://www.ibrtses.com/delphi/neuralnets.html</a><br>
</ul>
<p>

If you are using a small computer (PC, Mac, etc.) you may want to have
a look at the  Central Neural System Electronic Bulletin Board
(see question <a href="FAQ4.html#A16">"Other sources of information"</a>).
There are lots of small simulator packages.
Some of the CNS materials can also be found at
<a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/neural/cns/0.html"> 
http://www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/neural/cns/0.html</a>
<p>

<pre>
------------------------------------------------------------------------
</pre>

<H2>Subject: <a name="A18">Freeware and shareware packages for NN simulation?</a></H2>

Since the FAQ maintainer works for a software company, he does not
recommend or evaluate software in the FAQ. The descriptions below
are provided by the developers or distributors of the software. <p>

Note for future submissions: Please restrict product descriptions to a
maximum of 60 lines of 72 characters, in either plain-text format or,
preferably, HTML format. If you include the standard header (name,
company, address, etc.), you need not count the header in the 60 line
maximum.  Please confine your HTML to features that are supported by
primitive browsers, especially NCSA Mosaic 2.0; avoid tables, for example--use
&lt;pre&gt; instead.  Try to make the descriptions objective, and avoid making
implicit or explicit assertions about competing products, such as "Our
product is the *only* one that does so-and-so." The FAQ maintainer
reserves the right to remove excessive marketing hype and to edit
submissions to conform to size requirements; if he is in a good mood, he
may also correct your spelling and punctuation.  <P>

The following simulators are described below:
<OL>
<li> <A HREF="#JavaNNS">JavaNNS</A>
<li> <A HREF="#SNNS">SNNS</A>
<li> <A HREF="#PDP">PDP++</A>
<li> <A HREF="#RCS">Rochester Connectionist Simulator</A>
<li> <A HREF="#SFINX">UCLA-SFINX</A>
<li> <A HREF="#NeurDS">NeurDS</A>
<li> <A HREF="#PlaNet">PlaNet (formerly known as SunNet)</A>
<li> <A HREF="#GENESIS">GENESIS</A>
<li> <A HREF="#Mactivation">Mactivation</A>
<li> <A HREF="#CasCor">Cascade Correlation Simulator</A>
<li> <A HREF="#Quickprop">Quickprop</A>
<li> <A HREF="#Dartnet">DartNet</A>
<li> <A HREF="#AspirinMIGRAINES">Aspirin/MIGRAINES</A>
<li> <A HREF="#ALN">ALN Workbench</A>
<li> <A HREF="#Uts">Uts (Xerion, the sequel)</A>
<li> <A HREF="#MUME">Multi-Module Neural Computing Environment (MUME)</A>
<li> <A HREF="#LVQ_PAK">LVQ_PAK, SOM_PAK</A>
<li> <A HREF="#NevProp">Nevada Backpropagation (NevProp)</A>
<li> <A HREF="#Fuzzy-ARTmap">Fuzzy ARTmap</A>
<li> <A HREF="#PYGMALION">PYGMALION</A>
<li> <A HREF="#Basis-of-AI">Basis-of-AI-NN Software</A>
<li> <A HREF="#Matrix">Matrix Backpropagation</A>
<li> <A HREF="#BIOSIM">BIOSIM</A>
<li> <A HREF="#FuNeGen">FuNeGen</A>
<li> <A HREF="#NeuDL">NeuDL -- Neural-Network Description Language</A>
<li> <A HREF="#NeoC">NeoC Explorer</A>
<li> <A HREF="#AINET">AINET</A>
<li> <A HREF="#DemoGNG">DemoGNG</A>
<li> <A HREF="#Trajan">Trajan 2.1 Shareware</a>
<li> <A HREF="#Fingertips">Neural Networks at your Fingertips</a>
<li> <A HREF="#NNFit">NNFit</a>
<li> <A HREF="#Nenet">Nenet v1.0</a>
<li> <A href="FAQ6.html#MachineConsciousnessToolbox"> Machine Consciousness Toolbox</A>
<li> <A HREF="#NICO">NICO Toolkit (speech recognition)</a>
<li> <A HREF="#SOMtoolbox">SOM Toolbox for Matlab 5</a>
<li> <A HREF="#FastICA">FastICA package for MATLAB</a>
<li> <A HREF="#NEXUS">NEXUS: Large-scale biological simulations</a>
<li> <A HREF="#Netlab">Netlab: Neural network software for Matlab</a>
<li> <a href="#NuTank"> NuTank</A>
<li> <a href="#Lens"> Lens</A>
<li> <A HREF="#Joone">Joone: Java Object Oriented Neural Engine</A>
<li> <A HREF="#NV"> NV: Neural Viewer</A>
<li> <A HREF="#EasyNN"> EasyNN</A>
<li> <A HREF="#MLP-JAVA"> Multilayer Perceptron - A Java Implementation </a>
</OL>

See also
<a href="http://www.emsl.pnl.gov:2080/proj/neuron/neural/systems/shareware.html">
http://www.emsl.pnl.gov:2080/proj/neuron/neural/systems/shareware.html</a>
<p>


<ol>

<li><H3><A name="JavaNNS"> JavaNNS: Java Neural Network Simulator</a></H3>

<a href="http://www-ra.informatik.uni-tuebingen.de/forschung/JavaNNS/welcome_e.html">http://www-ra.informatik.uni-tuebingen.de/forschung/JavaNNS/welcome_e.html</a><br>

JavaNNS is the successor to <A HREF="#SNNS">SNNS</A>. JavaNNS is based
on the SNNS computing kernel, but has a newly developed graphical user
interface written in Java set on top of it. Hence compatibility with
SNNS is achieved while platform-independence is increased. <p>

In addition to SNNS features, JavaNNS offers the capability of linking
HTML browsers to it. This provides for accessing the user manual
(available in HTML) or, optionally, a reference coursebook on neural
networks directly from within the program. <p>

JavaNNS is available for Windows NT / Windows 2000, Solaris and RedHat
Linux. Additional ports are planed.  JavaNNS is freely available and can
be downloaded from the URL shown above. <p>

Contact: Igor Fischer, Phone: +49 7071 29-77176, fischer@informatik.uni-tuebingen.de <p>

<li><H3><a name="SNNS"> SNNS 4.2</a></H3>
   SNNS (Stuttgart Neural Network Simulator) is a software simulator for
   neural networks on Unix workstations developed at the Institute for
   Parallel and Distributed High Performance Systems (IPVR) at the
   University of Stuttgart. The goal of the SNNS project is to create an
   efficient and flexible simulation environment for research on and
   application of neural nets. <p>

   The SNNS simulator consists of two main components:<p>
   <OL>
   <LI>simulator kernel written in C<br>
   <LI>graphical user interface under X11R4 or X11R5
   </OL><p>

   The simulator kernel operates on the internal network data structures
   of the neural nets and performs all operations of learning and recall.
   It can also be used without the other parts as a C program embedded in
   custom applications. It supports arbitrary network topologies and,
   like RCS, supports the concept of sites. SNNS can be extended by the
   user with user defined activation functions, output functions, site
   functions and learning procedures, which are written as simple C
   programs and linked to the simulator kernel. C code can be generated
   from a trained network.<p>

   Currently the following network architectures and learning procedures
   are included:<p>

   <UL>
   <LI>Backpropagation (BP) for feedforward networks
           <UL>
           <LI>vanilla (online) BP
           <LI>BP with momentum term and flat spot elimination
           <LI>batch BP
           <LI>chunkwise BP
           </UL>
   <LI>Counterpropagation 
   <LI>Quickprop
   <LI>Backpercolation 1
   <LI>RProp
   <LI>Generalized radial basis functions (RBF)
   <LI>ART1
   <LI>ART2
   <LI>ARTMAP
   <LI>Cascade Correlation
   <LI>Dynamic LVQ
   <LI>Backpropagation through time (for recurrent networks)
   <LI>Quickprop through time (for recurrent networks)
   <LI>Self-organizing maps (Kohonen maps)
   <LI>TDNN (time-delay networks) with Backpropagation
   <LI>Jordan networks
   <LI>Elman networks and extended hierarchical Elman networks
   <LI>Associative Memory
   <LI>TACOMA
   </UL><p>

   The graphical user interface XGUI (X Graphical User Interface), built
   on top of the kernel, gives a 2D and a 3D graphical representation of
   the neural networks and controls the kernel during the simulation run.
   In addition, the 2D user interface has an integrated network editor
   which can be used to directly create, manipulate and visualize neural
   nets in various ways. <p>

   SNNSv4.1 has been tested on SUN SparcSt ELC,IPC (SunOS 4.1.2, 4.1.3), SUN
   SparcSt 2 (SunOS 4.1.2), SUN SparcSt 5, 10, 20 (SunOS 4.1.3, 5.2),
   DECstation 3100, 5000 (Ultrix V4.2), DEC Alpha AXP 3000 (OSF1 V2.1), IBM-PC
   80486, Pentium (Linux), IBM RS 6000/320, 320H, 530H (AIX V3.1, AIX V3.2),
   HP 9000/720, 730 (HP-UX 8.07), and SGI Indigo 2 (IRIX 4.0.5, 5.3). <p>

   The distributed kernel can spread one learning run over a workstation cluster. <p>

   SNNS web page: <a href="http://www-ra.informatik.uni-tuebingen.de/SNNS">http://www-ra.informatik.uni-tuebingen.de/SNNS</a><br>
   Ftp server: <a href="ftp://ftp.informatik.uni-tuebingen.de/pub/SNNS">ftp://ftp.informatik.uni-tuebingen.de/pub/SNNS</a><br>
   <ul>
   <li><a href="ftp://ftp.informatik.uni-tuebingen.de/pub/SNNS/SNNSv4.1.Readme">
     SNNSv4.1.Readme</a>  <br>
   <li><a href="ftp://ftp.informatik.uni-tuebingen.de/pub/SNNS/SNNSv4.1.tar.gz">
     SNNSv4.1.tar.gz (1.4 MB, Source code)</a> <br>
   <li><a href="ftp://ftp.informatik.uni-tuebingen.de/pub/SNNS/SNNSv4.1.Manual.ps.gz">
     SNNSv4.1.Manual.ps.gz (1 MB, Documentation)</a>
   </ul>
   Mailing list: <a href="http://www-ra.informatik.uni-tuebingen.de/SNNS/about-ml.html">http://www-ra.informatik.uni-tuebingen.de/SNNS/about-ml.html</a>
   <! Dipl.-Inform. Henri Siemens   Phone: (+49/0) 7071-29-78970, Fax:
      7071-922983
      University of Tuebingen,   WSI,  Computer Architecture,     
      Koestlinstr. 6,
      D-72074 Tuebingen, Germany              
      Siemens@Informatik.Uni-Tuebingen.DE >
   <p>


<li><H3><a name="PDP"> PDP++</a></H3>

URL: <a href="http://www.cnbc.cmu.edu/PDP++/PDP++.html">http://www.cnbc.cmu.edu/PDP++/PDP++.html</a>
<p>

The PDP++ software is a neural-network simulation system written
in C++.  It represents the next generation of the PDP software
released with the McClelland and Rumelhart "Explorations in Parallel
Distributed Processing Handbook", MIT Press, 1987.  It is easy enough
for novice users, but very powerful and flexible for research use.
PDP++ is featured in a new textbook,
<A HREF="http://psych.colorado.edu/~oreilly/comp_ex_cog_neuro.html">
<cite>Computational Explorations in Cognitive Neuroscience:
Understanding the Mind by Simulating the Brain,</cite></a>
by <A HREF="http://psych.colorado.edu/~oreilly"> Randall C. O'Reilly</A>
and <A HREF="http://kore.psy.du.edu/munakata/index.html">Yuko Munakata,</A>
<A HREF="http://mitpress.mit.edu">MIT Press</A>, 2000.<P>

Supported algorithms include: <p>
<UL>
<LI> Feedforward and recurrent error backpropagation.  Recurrent BP
    includes continuous, real-time models, and Almeida-Pineda.
<LI> Constraint satisfaction algorithms and associated learning
    algorithms including Boltzmann Machine, Hopfield models,
    mean-field networks (DBM), Interactive Activation and
    Competition (IAC), and continuous stochastic networks.
<LI>  Self-organizing learning including Competitive Learning, Soft
    Competitive Learning, simple Hebbian, and Self-organizing Maps
    ("Kohonen Nets").
<LI> Mixtures-of-experts using backpropagation experts, EM updating,
    and a SoftMax gating module.
<LI> Leabra algorithm that combines error-driven and Hebbian learning
    with k-Winners-Take-All inhibitory competition.
</UL><p>


The software can be obtained by anonymous ftp from: <ul>
<li> <a href="ftp://grey.colorado.edu/pub/oreilly/pdp++">ftp://grey.colorado.edu/pub/oreilly/pdp++</a> or <br>
<li> <a href="ftp://cnbc.cmu.edu/pub/pdp++/">ftp://cnbc.cmu.edu/pub/pdp++/</a>  or <br>
<li> <a href="ftp://unix.hensa.ac.uk/mirrors/pdp++/">ftp://unix.hensa.ac.uk/mirrors/pdp++/</a>
</ul><p>

<! email: Randall C. O'Reilly" oreilly@psych.colorado.edu>
<p>

<li><H3><A name="RCS"> Rochester Connectionist Simulator</a></H3>
   A versatile simulator program for arbitrary types of
   neural nets. Comes with a backprop package and a X11/Sunview
   interface.
   Available via anonymous FTP from
   <a href="ftp://ftp.cs.rochester.edu/pub/packages/simulator/simulator_v4.2.tar.Z"> 
   ftp://ftp.cs.rochester.edu/pub/packages/simulator/simulator_v4.2.tar.Z</a>
   There's also a patch available from
   <a href="ftp://ftp.cs.rochester.edu/pub/packages/simulator/simulator_v4.2.patch.1">
   ftp://ftp.cs.rochester.edu/pub/packages/simulator/simulator_v4.2.patch.1</a>
   <p>

<li><H3><a name="SFINX"> UCLA-SFINX</a></H3>
   The UCLA-SFINX, a "neural" network simulator is now in public domain.
   UCLA-SFINX (Structure and Function In Neural connec- tions) is an
   interactive neural network simulation environment designed to provide
   the investigative tools for studying the behavior of various neural
   structures. It was designed to easily express and simulate the highly
   regular patterns often found in large networks, but it is also
   general enough to model parallel systems of arbitrary
   interconnectivity. For more information, see
   <a href="http://decus.acornsw.com/vs0121/AISIG/F90/NETS/UCLA_SIM.TXT">http://decus.acornsw.com/vs0121/AISIG/F90/NETS/UCLA_SIM.TXT</a>
   <p>

<li><H3><a name="NeurDS"> NeurDS</a></H3>
   Neural Design and Simulation System. This is a general purpose
   tool for building, running and analysing Neural Network Models in
   an efficient manner. NeurDS will compile and run virtually any
   Neural Network Model using a consistent user interface that may
   be either window or "batch" oriented.
   HP-UX 8.07 source code is available from
   <a href="http://hpux.u-aizu.ac.jp/hppd/hpux/NeuralNets/NeurDS-3.1/">
   http://hpux.u-aizu.ac.jp/hppd/hpux/NeuralNets/NeurDS-3.1/</a> or
   <a href="http://askdonna.ask.uni-karlsruhe.de/hppd/hpux/NeuralNets/NeurDS-3.1/">
   http://askdonna.ask.uni-karlsruhe.de/hppd/hpux/NeuralNets/NeurDS-3.1/</a>
   <p>

<li><H3><a name="PlaNet"> PlaNet5.7 (formerly known as SunNet)</a></H3>
   A popular connectionist simulator with versions to
   run under X Windows, and non-graphics terminals
   created by Yoshiro Miyata (Chukyo Univ., Japan).
   60-page User's Guide in Postscript.
   Send any questions to miyata@sccs.chukyo-u.ac.jp
   Available for anonymous ftp from ftp.ira.uka.de
   as <a href="ftp://ftp.ira.uka.de/pub/neuron/PlaNet5.7.tar.gz">
    /pub/neuron/PlaNet5.7.tar.gz (800 kb)</a> <p>

<li><H3><a name="GENESIS"> GENESIS</a></H3>
   GENESIS 2.0 (GEneral NEural SImulation System) is a general purpose
   simulation platform which was developed to support the simulation of
   neural systems ranging from complex models of single neurons to
   simulations of large networks made up of more abstract neuronal
   components.  Most current GENESIS applications involve realistic
   simulations of biological neural systems.  Although the software can
   also model more abstract networks, other simulators are more suitable
   for backpropagation and similar connectionist modeling.
   Runs on most Unix platforms. Graphical front end XODUS.
   Parallel version for networks of
   workstations, symmetric multiprocessors, and MPPs also available.
   Further information via WWW at
   <A HREF="http://www.genesis-sim.org/GENESIS/">http://www.genesis-sim.org/GENESIS/</A>.
   <p>

<li><H3><a name="Mactivation"> Mactivation</a></H3>
   A neural network simulator for the Apple Macintosh.
   Available for ftp from ftp.cs.colorado.edu
   as <a href="ftp://ftp.cs.colorado.edu/pub/cs/misc/Mactivation-3.3.sea.hqx">
     /pub/cs/misc/Mactivation-3.3.sea.hqx</a> <p>

<li><H3><a name="CasCor"> Cascade Correlation Simulator</a></H3>
   A simulator for Scott Fahlman's Cascade Correlation algorithm.
   Available for ftp from ftp.cs.cmu.edu
   in directory  /afs/cs/project/connect/code/supported as the file
   <a href="ftp://ftp.cs.cmu.edu/afs/cs/project/connect/code/cascor-v1.2.shar">
     cascor-v1.2.shar (223 KB)</a>
   There is also a version of recurrent cascade correlation in the same
   directory in file
   <a href="ftp://ftp.cs.cmu.edu/afs/cs/project/connect/code/supported/rcc1.c">
     rcc1.c (108 KB)</a>. <p>

<li><H3><a name="Quickprop"> Quickprop</a></H3>
   A variation of the back-propagation algorithm developed by
   Scott Fahlman.  A simulator is available in the same directory
   as the cascade correlation simulator above in file
   <a href="ftp://ftp.cs.cmu.edu/afs/cs/project/connect/code/unsupported/nevprop1.16.shar">
     nevprop1.16.shar (137 KB)</a><br>
   (There is also an obsolete simulator called
   <a href="ftp://ftp.cs.cmu.edu/afs/cs/project/connect/code/unsupported/quickprop1.c">
     quickprop1.c (21 KB)</a> in the
    same directory, but it has been superseeded by NevProp. See also the
    description of <A HREF="#NevProp">NevProp</A> below.) <p>

<li><H3><a name="Dartnet"> DartNet</a></H3>
   DartNet is a Macintosh-based backpropagation simulator, developed
   at Dartmouth by Jamshed Bharucha and Sean Nolan as a pedagogical tool.
   It makes use of the Mac's graphical interface, and provides a number
   of tools for building, editing, training, testing and examining
   networks. This program is available by anonymous ftp from
   ftp.dartmouth.edu as
   <a href="ftp://ftp.dartmouth.edu/pub/mac/dartnet.sit.hqx">
     /pub/mac/dartnet.sit.hqx (124 KB)</a>. <p>

<li><H3><a name="AspirinMIGRAINES"> Aspirin/MIGRAINES</a></H3>
   Aspirin/MIGRAINES 6.0 consists of a code generator that builds neural network
   simulations by reading a network description (written in a language
   called "Aspirin") and generates a C simulation. An interface
   (called "MIGRAINES") is provided to export data from the neural
   network to visualization tools.
   The system has been ported to a large number of platforms.
   The goal of Aspirin is to provide a common extendible front-end language
   and parser for different network paradigms.
   The MIGRAINES interface is a terminal based interface
   that allows you to open Unix pipes to data in the neural
   network. Users can display the data using
   either public or commercial graphics/analysis tools.
   Example filters are included that convert data exported through
   MIGRAINES to formats readable by Gnuplot 3.0, Matlab, Mathematica,
   and xgobi. <p>
   The software is available from
   <a href="http://www.elegant-software.com/software/aspirin/">http://www.elegant-software.com/software/aspirin/</a>
   <p>

<li><H3><a name="ALN"> ALN Workbench (a spreadsheet for Windows)</a>
</H3>

ALNBench is a free spreadsheet program for MS-Windows (NT, 95) that
allows the user to import training and test sets and predict a chosen
column of data from the others in the training set.  It is an
easy-to-use program for research, education and evaluation of ALN
technology.  Anyone who can use a spreadsheet can quickly understand how
to use it.  It facilitates interactive access to the power of the
<a href="http://www.dendronic.com/beta.htm">Dendronic Learning Engine (DLE),</a>
a product in commercial use. <p>

An ALN consists of linear functions with adaptable weights at the leaves
of a tree of maximum and minimum operators. The tree grows automatically
during training: a linear piece splits if its error is too high.  The
function computed by an ALN is piecewise linear and continuous.  It can
learn to approximate any continuous function to arbitrarily high
accuracy. <p>

Parameters allow the user to input knowledge about a function to promote
good generalization.  In particular, bounds on the weights of the linear
functions can be directly enforced.  Some parameters are chosen
automatically in standard mode, and are under user control in expert
mode.  <P>

The program can be downloaded from 
<a href="http://www.dendronic.com/main.htm">http://www.dendronic.com/main.htm</a>
<p>
   
For further information please contact:
<pre>
William W. Armstrong PhD, President
Dendronic Decisions Limited
3624 - 108 Street, NW
Edmonton, Alberta,
Canada T6J 1B4
Email: arms@dendronic.com
URL: <a href="http://www.dendronic.com/">http://www.dendronic.com/</a>
Tel. +1 403 421 0800
(Note: The area code 403 changes to 780 after Jan. 25, 1999)
</pre> <p>
   


<li><H3><a name="Uts"> Uts (Xerion, the sequel)</a></H3>
   Uts is a portable artificial neural network simulator written on top
   of the Tool Control Language (Tcl) and the Tk UI toolkit. As result,
   the user interface is readily modifiable and it is possible to simultaneously
   use the graphical user interface and visualization tools and use
   scripts written in Tcl. Uts itself implements only the connectionist
   paradigm of linked units in Tcl and the basic elements of the graphical
   user interface. To make a ready-to-use package, there exist modules
   which use Uts to do back-propagation (tkbp) and mixed em gaussian optimization
   (tkmxm). Uts is available in ftp.cs.toronto.edu in directory /pub/xerion.
   <p>


<li><H3><a name="MUME"> Multi-Module Neural Computing Environment (MUME)</a></H3>
   MUME is a simulation environment for multi-modules neural computing. It
   provides an object oriented facility for the simulation and training
   of multiple nets with various architectures and learning algorithms.
   MUME includes a library of network architectures including feedforward,
   simple recurrent, and continuously running recurrent neural networks.
   Each architecture is supported by a variety of learning algorithms.
   MUME can be used for large scale neural network simulations as it provides
   support for learning in multi-net environments. It also provide pre- and
   post-processing facilities. For more information, see
   <a href="http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/mume/0.html">
   http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/mume/0.html</a>
   <p>


<li><H3><a name="LVQ_PAK"> LVQ_PAK, SOM_PAK</a></H3>
   These are packages for Learning Vector Quantization and
   Self-Organizing Maps, respectively.
   They have been built by the LVQ/SOM Programming Team of the
   Helsinki University of Technology, Laboratory of Computer and
   Information Science, Rakentajanaukio 2 C, SF-02150 Espoo, FINLAND
   There are versions for Unix and MS-DOS available from
   <a href="http://nucleus.hut.fi/nnrc/nnrc-programs.html">http://nucleus.hut.fi/nnrc/nnrc-programs.html</a>
   <p>

<li><H3><a name="NevProp"> Nevada Backpropagation (NevProp)</a></H3>

NevProp, version 3, is a relatively easy-to-use, feedforward
backpropagation multilayer perceptron simulator-that is, statistically
speaking, a multivariate nonlinear regression program. NevProp3 is
distributed for free under the terms of the GNU Public License and
can be downloaded from 
<a href="http://brain.cs.unr.edu/publications/NevProp.zip">
         http://brain.cs.unr.edu/publications/NevProp.zip</a>
and
<a href="http://brain.cs.unr.edu/publications/NevPropManual.pdf">
         http://brain.cs.unr.edu/publications/NevPropManual.pdf</a>
<p>
The program is distributed as C source code that should compile and run
on most platforms. In addition, precompiled executables are available
for Macintosh and DOS platforms. Limited support is available from Phil
Goodman (goodman@unr.edu), University of Nevada Center for Biomedical
Research. <p>
MAJOR FEATURES OF NevProp3 OPERATION (* indicates feature new in version 3) <ol>
<li> Character-based interface common to the UNIX, DOS, and Macintosh
platforms.
<li> Command-line argument format to efficiently initiate NevProp3. For
Generalized Nonlinear Modeling (GNLM) mode, beginners may opt to use an
interactive interface.
<li> Option to pre-standardize the training data (z-score or forced
range*).
<li> Option to pre-impute missing elements in training data (case-wise
deletion, or imputation with mean, median, random selection, or k-nearest
neighbor).*
<li> Primary error (criterion) measures include mean square error,
hyperbolic tangent error, and log likelihood (cross-entropy), as penalized
an unpenalized values. 
<li> Secondary measures include ROC-curve area (c-index), thresholded
classification, R-squared and Nagelkerke R-squared. Also reported at
intervals are the weight configuration, and the sum of square weights. 
<li> Allows simultaneous use of logistic (for dichotomous outputs) and
linear output activation functions (automatically detected to assign
activation and error function).*
<li> 1-of-N (Softmax)* and M-of-N options for binary classification. 
<li> Optimization options: flexible learning rate (fixed global adaptive,
weight-specific, quickprop), split learn rate (inversely proportional to
number of incoming connections), stochastic (case-wise updating),
sigmoidprime offset (to prevent locking at logistic tails).
<li> Regularization options: fixed weight decay, optional decay on bias
weights, Bayesian hyperpenalty* (partial and full Automatic Relevance
Determination-also used to select important predictors), automated early
stopping (full dataset stopping based on multiple subset cross-validations)
by error criterion.
<li> Validation options: upload held-out validation test set; select
subset of outputs for joint summary statistics;* select automated
bootstrapped modeling to correct optimistically biased summary statistics
(with standard deviations) without use of hold-out.
<li> Saving predictions: for training data and uploaded validation test
set, save file with identifiers, true targets, predictions, and (if
bootstrapped models selected) lower and upper 95% confidence limits* for
each prediction.
<li> Inference options: determination of the mean predictor effects and
level effects (for multilevel predictor variables); confidence limits within
main model or across bootstrapped models.*
<li> ANN-kNN (k-nearest neighbor) emulation mode options: impute missing
data elements and save to new data file; classify test data (with or without
missing elements) using ANN-kNN model trained on data with or without
missing elements (complete ANN-based expectation maximization).*
<li> AGE (ANN-Gated Ensemble) options: adaptively weight predictions (any
scale of scores) obtained from multiple (human or computational) "experts";
validate on new prediction sets; optional internal prior-probability
expert.* 
</ol> <p>

<li><H3><a name="Fuzzy-ARTmap"> Fuzzy ARTmap</a></H3>
   This is just a small example program.
   Available for anonymous ftp from park.bu.edu [128.176.121.56]
   <a href="ftp://cns-ftp.bu.edu/pub/fuzzy-artmap.tar.Z">
     ftp://cns-ftp.bu.edu/pub/fuzzy-artmap.tar.Z (44 kB)</a>. <p>

<li><H3><a name="PYGMALION"> PYGMALION</a></H3>
   This is a prototype that stems from an ESPRIT project. It implements
   back-propagation, self organising map, and Hopfield nets.
   Avaliable for ftp from ftp.funet.fi [128.214.248.6] as
   <a href="ftp://ftp.funet.fi/pub/sci/neural/sims/pygmalion.tar.Z">
     /pub/sci/neural/sims/pygmalion.tar.Z (1534 kb)</a>.
   (Original site is imag.imag.fr:
   <a href="ftp://imag.imag.fr/archive/pygmalion/pygmalion.tar.Z">
     archive/pygmalion/pygmalion.tar.Z</a>). <p>


<li><H3><a name="Basis-of-AI"> Basis-of-AI-NN Software</a></H3>

   Non-GUI DOS and UNIX source code, DOS binaries and examples
   are available in the following different program sets and the
   backprop package has a Windows 3.x binary and a Unix/Tcl/Tk
   version:

   <pre>
   [backprop, quickprop, delta-bar-delta, recurrent networks],
   [simple clustering, k-nearest neighbor, LVQ1, DSM],
   [Hopfield, Boltzman, interactive activation network],
   [interactive activation network],
   [feedforward counterpropagation],
   [ART I],
   [a simple BAM] and
   [the linear pattern classifier]
   </pre>
   
   For details see: <a href="http://www.dontveter.com/nnsoft/nnsoft.html">http://www.dontveter.com/nnsoft/nnsoft.html</a>
   
   <p>
   An improved professional version of backprop is also available;
   see <a href="FAQ6.html#Prof-Basis-of-AI">Part 6</a> of the FAQ.
   
   <p>
   Questions to:  Don Tveter, don@dontveter.com
   <p>

<li><H3><a name="Matrix"> Matrix Backpropagation</a></H3>
   MBP (Matrix Back Propagation) is a very efficient implementation of the
   back-propagation algorithm for current-generation workstations.  The
   algorithm includes a per-epoch adaptive technique for gradient
   descent.  All the computations are done through matrix multiplications
   and make use of highly optimized C code. The goal is to reach almost
   peak-performances on RISCs with superscalar capabilities and fast
   caches.  On some machines (and with large networks) a 30-40x speed-up
   can be measured with respect to conventional implementations.
   The software is available by anonymous ftp from
      ftp.esng.dibe.unige.it as
   <a href="ftp://ftp.esng.dibe.unige.it/neural/MBP/MBPv1.1.tar.Z">
      /neural/MBP/MBPv1.1.tar.Z</a> (Unix version), or
   <a href="ftp://ftp.esng.dibe.unige.it/neural/MBP/MBPv11.zip">
      /neural/MBP/MBPv11.zip</a> (PC version).,
   For more information, contact Davide Anguita (anguita@dibe.unige.it).
   <p>


<li><H3><a name="BIOSIM"> BIOSIM</a></H3>
   BIOSIM is a biologically oriented neural network simulator.
   Public domain, runs on Unix (less powerful PC-version is available, too),
   easy to install, bilingual (german and english),
   has a GUI (Graphical User Interface), designed for research and teaching,
   provides online help facilities, offers controlling interfaces,
   batch version is available, a DEMO is provided. <p>

   REQUIREMENTS (Unix version): X11 Rel. 3 and above, Motif Rel 1.0 and above,
     12 MB of physical memory, recommended are 24 MB and more, 20 MB disc space.
   REQUIREMENTS (PC version): PC-compatible with MS Windows 3.0 and above,
     4 MB of physical memory, recommended are 8 MB and more, 1 MB disc space. <p>

   Four neuron models are implemented in BIOSIM: a simple model only
   switching ion channels on and off, the original Hodgkin-Huxley model,
   the SWIM model (a modified HH model) and the Golowasch-Buchholz model.
   Dendrites consist of a chain of segments without bifurcation.
   A neural network can be created by using the interactive network
   editor which is part of BIOSIM. Parameters can be changed via context
   sensitive menus and the results of the simulation can be visualized
   in observation windows for neurons and synapses.
   Stochastic processes such as noise can be included.
   In addition, biologically orientied learning and forgetting processes
   are modeled, e.g. sensitization, habituation, conditioning, hebbian
   learning and competitive learning.
   Three synaptic types are predefined (an excitatatory synapse type,
   an inhibitory synapse type and an electrical synapse).
   Additional synaptic types can be created interactively as desired. <p>

   Available for ftp from ftp.uni-kl.de in directory
   /pub/bio/neurobio: Get
   <a href="ftp://ftp.uni-kl.de/pub/bio/neurobio/biosim.readme">
   /pub/bio/neurobio/biosim.readme (2 kb)</a> and
   <a href="ftp://ftp.uni-kl.de/pub/bio/neurobio/biosim.tar.Z">
   /pub/bio/neurobio/biosim.tar.Z (2.6 MB)</a> for the Unix version or
   <a href="ftp://ftp.uni-kl.de/pub/bio/neurobio/biosimpc.readme">
   /pub/bio/neurobio/biosimpc.readme (2 kb)</a> and
   <a href="ftp://ftp.uni-kl.de/pub/bio/neurobio/biosimpc.zip">
   /pub/bio/neurobio/biosimpc.zip (150 kb)</a> for the PC version. <p>
   Contact: <br>
     Stefan Bergdoll <br>
     Department of Software Engineering (ZXA/US) <br>
     BASF Inc. <br>
     D-67056 Ludwigshafen; Germany <br>
     bergdoll@zxa.basf-ag.de  phone 0621-60-21372 fax 0621-60-43735 <br>
    <p>


<li><H3><a name="FuNeGen"> FuNeGen 1.0</a></H3>
  FuNeGen is a MLP based software program to generate fuzzy rule based
  classifiers. For more information, see
  <a href="http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/fuzzy/systems/funegen/">
  http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/fuzzy/systems/funegen/</a>
  <p>


<li><H3><a name="NeuDL"> NeuDL -- Neural-Network Description Language</a></H3>
  NeuDL is a description language for the design, training, and
  operation of neural networks.  It is currently limited to the
  backpropagation neural-network model; however, it offers a
  great deal of flexibility.  For example, the user can explicitly
  specify the connections between nodes and can create or destroy
  connections dynamically as training progresses. NeuDL is an
  interpreted language resembling C or C++.  It also has instructions
  dealing with training/testing set manipulation as well as neural
  network operation.  A NeuDL program can be run in interpreted mode or
  it can be automatically translated into C++ which can be compiled and
  then executed.  The NeuDL interpreter is written in C++ and can be
  easly extended with new instructions. For more information, see
  <a href="http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/neudl/0.html">
  http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/neudl/0.html</a> 
  <p>


<li><H3><a name="NeoC"> NeoC Explorer (Pattern Maker included)</a></H3>
   The NeoC software is an implementation of Fukushima's Neocognitron
   neural network. Its purpose is to test the model and to facilitate
   interactivity for the experiments. Some substantial features: GUI,
   explorer and tester operation modes, recognition statistics,
   performance analysis, elements displaying, easy net construction.
   PLUS, a pattern maker utility for testing ANN: GUI, text file output,
   transformations. For more information, see
   <a href="http://www.simtel.net/pub/pd/39893.html">http://www.simtel.net/pub/pd/39893.html</a>
   <p>

<li><H3><a name="AINET"> AINET</a></H3>

  AINET is a probabilistic neural network application which runs on
  Windows 95/NT. It was designed specifically to facilitate the
  modeling task in all neural network problems. It is lightning fast
  and can be used in conjunction with many different programming
  languages.  It does not require iterative learning, has no limits in
  variables (input and output neurons), no limits in sample size. It
  is not sensitive toward noise in the data. The database can be
  changed dynamically. It provides a way to estimate the rate of error
  in your prediction. It has a graphical spreadsheet-like user
  interface. The AINET manual (more than 100 pages) is divided into:
  "User's Guide", "Basics About Modeling with the AINET", "Examples",
  "The AINET DLL library" and "Appendix" where the theoretical
  background is revealed.  You can get a full working copy from:  <a
  href="http://www.ainet-sp.si/">http://www.ainet-sp.si/</a> <p>

<! Ales Krajnc
o------------------o------------------------------------o
| aiNet            | Tel. : +386 63 49 00 680           |
| Trubarjeva 42    | Fax  :                             |
| SI - 3000 Celje  | email: ainet@siol.net              |
| Slovenia         | WWW  : http://www.ainet-sp.si      |
| Europe           | (www is still under construction)  |
o------------------o------------------------------------o
>



<li><H3><a name="DemoGNG">DemoGNG</a></H3> 
  This simulator is written in Java and
  should therefore run without compilation on all platforms where a Java
  interpreter (or a browser with Java support) is available. It implements the
  following algorithms and neural network models:
   
  <UL>
  <LI> Hard Competitive Learning (standard algorithm)
  <LI> Neural Gas (Martinetz and Schulten 1991)
  <LI> Competitive Hebbian Learning (Martinetz and Schulten 1991, Martinetz 1993)
  <LI> Neural Gas with Competitive Hebbian Learning (Martinetz and Schulten 1991)
  <LI> Growing Neural Gas (Fritzke 1995)
  </UL>
   
  DemoGNG is distributed under the GNU General Public License. It allows to
  experiment with the different methods using various probability
  distributions. All model parameters can be set interactively on the graphical
  user interface. A teach modus is provided to observe the models in
  "slow-motion" if so desired. It is currently <B>not</B> possible to experiment
  with user-provided data, so the simulator is useful basically for
  demonstration and teaching purposes and as a sample implementation of the
  above algorithms.
   <P> 
  DemoGNG can be accessed most easily at <A HREF="http://www.neuroinformatik.ruhr-uni-bochum.de/">http://www.neuroinformatik.ruhr-uni-bochum.de/</A> 
  in the file <A HREF="http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/research/gsn/DemoGNG/GNG.html">/ini/VDM/research/gsn/DemoGNG/GNG.html</A> where
  it is embedded as Java applet into a Web page and is downloaded for immediate
  execution when you visit this page.
   
  An accompanying paper entitled "Some competitive learning methods" describes
  the implemented models in detail and is available in html at the same server
  in the directory
   <A HREF="http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/research/gsn/JavaPaper/">ini/VDM/research/gsn/JavaPaper/</A>.
  
   <P> It is also possible to download the complete source
  code and a Postscript version of the paper via anonymous ftp from
  ftp.neuroinformatik.ruhr-uni-bochum [134.147.176.16] in directory /pub/software/NN/DemoGNG/.
  The software is in the file <A
  HREF="ftp://ftp.neuroinformatik.ruhr-uni-bochum.de/pub/software/NN/DemoGNG/DemoGNG-1.00.tar.gz">DemoGNG-1.00.tar.gz</A>
  (193 KB) and the paper in the file <A
  HREF="ftp://ftp.neuroinformatik.ruhr-uni-bochum.de/pub/software/NN/DemoGNG/sclm.ps.gz">sclm.ps.gz</A>
  (89 KB). There is also a <A HREF="ftp://ftp.neuroinformatik.ruhr-uni-bochum.de/pub/software/NN/DemoGNG/README">README file</A> (9 KB). 
  Please send any comments and questions to <A
  HREF="mailto:demogng@neuroinformatik.ruhr-uni-bochum.de">demogng@neuroinformatik.ruhr-uni-bochum.de</A> 
  which will reach Hartmut Loos who has written DemoGNG as well as Bernd Fritzke, the author of the accompanying paper.
  <p>


<li><H3><A name="Trajan">Trajan 2.1 Shareware</a></H3>
Trajan 2.1 Shareware is a Windows-based Neural Network simulation
package. It includes support for the two most popular forms of
Neural Network:
Multilayer Perceptrons with Back Propagation and Kohonen networks.<p>

Trajan 2.1 Shareware concentrates on ease-of-use and feedback. It
includes Graphs, Bar Charts and Data Sheets presenting a range of
Statistical feedback in a simple, intuitive form. It also features
extensive on-line Help.<p>

The Registered version of the package can support very large networks
(up to 128 layers with up to 8,192 units each,
subject to memory limitations in the machine), and allows
simple Cut and Paste transfer of data to/from other Windows-packages,
such as spreadsheet programs. The Unregistered version features
limited network size and no Clipboard Cut-and-Paste.<p>

There is also a Professional version of Trajan 2.1, which supports a
wider range of network models, training algorithms and other
features.<p>

See Trajan Software's Home Page at
<a href="http://www.trajan-software.demon.co.uk">
http://www.trajan-software.demon.co.uk</a> for
further details, and a free copy of the Shareware version.<p>
Alternatively, email <a href=mailto:andrew@trajan-software.demon.co.uk>
andrew@trajan-software.demon.co.uk</a> for more details. <p>


<li><H3><a name="Fingertips">Neural Networks at your Fingertips</a></H3>
"Neural Networks at your Fingertips"
is a package of ready-to-reuse neural network simulation source code
which was prepared for educational purposes by Karsten Kutza.
The package consists of eight programs,
each of which implements a particular network architecture
together with an embedded example application
from a typical application domain.<br>
Supported network architectures are
<ul>
<li>Adaline,
<li>Backpropagation,
<li>Hopfield Model,
<li>Bidirectional Associative Memory,
<li>Boltzmann Machine,
<li>Counterpropagation,
<li>Self-Organizing Map, and
<li>Adaptive Resonance Theory.
</ul>
The applications demonstrate use of the networks in various domains
such as pattern recognition, time-series forecasting, associative
memory, optimization, vision, and control and include e.g. a sunspot
prediction, the traveling salesman problem, and a pole balancer.<br>
The programs are coded in portable, self-contained ANSI C
and can be obtained from the web pages at
<A HREF="http://www.geocities.com/CapeCanaveral/1624">
http://www.geocities.com/CapeCanaveral/1624</A>. <p>

<!   Dipl.-Inform. Karsten Kutza
     VW-GEDAS mbH                           Phone: +49/30/39971-326
     Pascalstra=DFe 11                        Fax:   +49/30/39971-999
     D-10587 Berlin                         Email: kk@gedas.de
>


<li><H3><a name="NNFit">NNFit</a></H3>

NNFit (Neural Network data Fitting) is a user-friendly software that
allows the development of empirical correlations between input and
output data.  Multilayered neural models have been implemented using a
quasi-newton method as learning algorithm. Early stopping method is
available and various tables and figures are provided to evaluate
fitting performances of the neural models. The software is available for
most of the Unix platforms with X-Windows (IBM-AIX, HP-UX, SUN, SGI,
DEC, Linux). Informations, manual and executable codes (english and
french versions) are available at <a href="http://www.gch.ulaval.ca/~nnfit">http://www.gch.ulaval.ca/~nnfit</a><br>
Contact: Bernard P.A. Grandjean, department of chemical engineering,<br>
Laval University; Sainte-Foy (Quibec) Canada G1K 7P4;<br>
grandjean@gch.ulaval.ca <p>


<li><H3><a name="Nenet">Nenet v1.0</a></H3>

Nenet v1.0 is a 32-bit Windows 95 and Windows NT 4.0 application
designed to facilitate the use of a Self-Organizing Map (SOM) algorithm. <p>

The major motivation for Nenet was to create a user-friendly SOM
algorithm tool with good visualization capabilities and with a GUI
allowing efficient control of the SOM parameters. The use scenarios have
stemmed from the user's point of view and a considerable amount of work
has been placed on the ease of use and versatile visualization methods. <p>

With Nenet, all the basic steps in map control can be performed. In
addition, Nenet also includes some more exotic and involved features
especially in the area of visualization. <p>

Features in Nenet version 1.0:
<ul>
<li> Implements the standard Kohonen SOM algorithm
<li> Supports 2 common data preprocessing methods
<li> 5 different visualization methods with rectangular or hexagonal
     topology
<li> Capability to animate both train and test sequences in all
     visualization methods
<li> Labelling
    <ul>
    <li> Both neurons and parameter levels can be labelled
    <li> Provides also autolabelling
    </ul>
<li> Neuron values can be inspected easily
<li> Arbitrary selection of parameter levels can be visualized with
     Umatrix simultaneously
<li> Multiple views can be opened on the same map data
<li> Maps can be printed
<li> Extensive help system provides fast and accurate online help
<li> SOM_PAK compatible file formats
<li> Easy to install and uninstall
<li> Conforms to the common Windows 95 application style - all
    functionality in one application
</ul> 
<p>

Nenet web site is at: 
<a href="http://www.mbnet.fi/~phodju/nenet/Nenet/General.html">http://www.mbnet.fi/~phodju/nenet/Nenet/General.html</a>
The web site contains further information on Nenet and also the
downloadable Nenet files (3 disks totalling about 3 Megs) <p>

If you have any questions whatsoever, please contact:
Nenet-Team@hut.fi  or   phassine@cc.hut.fi
<p>


<li><H3><A name="MachineConsciousnessToolbox"> Machine Consciousness
Toolbox</a></H3>

See listing for
<A href="FAQ6.html#MachineConsciousnessToolbox"> Machine Consciousness Toolbox</A>
in part 6 of the FAQ. <p>


<li><H3><a name="NICO">NICO Toolkit (speech recognition)</a></H3>

<pre>
      Name: NICO Artificial Neural Network Toolkit
    Author: Nikko Strom
   Address: Speech, Music and Hearing, KTH, S-100 44, Stockholm, Sweden
     Email: nikko@speech.kth.se
       URL: <a href="http://www.speech.kth.se/NICO/index.html">http://www.speech.kth.se/NICO/index.html</a>
 Platforms: UNIX, ANSI C; Source code tested on: HPUX, SUN Solaris, Linux
     Price: Free
</pre>

The NICO Toolkit is an artificial neural network toolkit designed and
optimized for automatic speech recognition applications. Networks with
both recurrent connections and time-delay windows are easily
constructed. The network topology is very flexible -- any number of
layers is allowed and layers can be arbitrarily connected. Sparse
connectivity between layers can be specified. Tools for extracting
input-features from the speech signal are included as well as tools for
computing target values from several standard phonetic label-file
formats. <p>

Algorithms:
<ul>
<li> Back-propagation through time,
<li> Speech feature extraction (Mel cepstrum coefficients, filter-bank)
</ul>

<p>


<li><H3><a name="SOMtoolbox">SOM Toolbox for Matlab 5</a></H3>

<P> SOM Toolbox, a shareware Matlab 5 toolbox for data analysis with
self-organizing maps is available at the URL 
<A href="http://www.cis.hut.fi/projects/somtoolbox/">
http://www.cis.hut.fi/projects/somtoolbox/</A>. If you are interested
in practical data analysis and/or self-organizing maps and have Matlab
5 in your computer, be sure to check this out!
      
<P>Highlights of the SOM Toolbox include the following:
<UL>
      <LI> Tools for all the stages of data analysis: besides the
      basic SOM training and visualization tools, the package
      includes also tools for data preprocessing and model
      validation and interpretation. 

      <LI> Graphical user interface (GUI): the GUI first guides
      the user through the initialization and training procedures, 
      and then offers a variety of different methods to
      visualize the data on the trained map.

      <LI> Modular programming style: the Toolbox code utilizes Matlab
      structures, and the functions are constructed in a modular
      manner, which makes it convenient to tailor the code for each
      user's specific needs.

      <LI> Advanced graphics: building on the Matlab's strong graphics
      capabilities, attractive figures can be easily produced.

      <LI> Compatibility with SOM_PAK: import/export functions for
      SOM_PAK codebook and data files are included in the package.

      <LI> Component weights and names: the input vector components
      may be given different weights according to their relative
      importance, and the components can be given names to make the
      figures easier to read.

      <LI> Batch or sequential training: in data analysis
      applications, the speed of training may be considerably improved
      by using the batch version.

      <LI> Map dimension: maps may be N-dimensional (but
      visualization is not supported when N &gt; 2 ).
</UL><p>


<li><h3><A name="FastICA">FastICA package for MATLAB</a></h3>

The FastICA algorithm for independent component analysis.
<p>
Independent component analysis, or ICA, is neural network or
signal processing technique that represents a multidimensional random
vector as a linear combination of nongaussian random variables
('independent components') that are as independent as possible.
ICA is a nongaussian version of factor analysis, and somewhat similar
to principal component analysis. ICA has many applications in data
analysis, source separation, and feature extraction.  
<p>
The FastICA algorithm is a computationally optimized method for
performing the estimation of ICA. It uses a fixed-point 
iteration scheme that has been found in independent experiments
to be 10-100 times faster than conventional gradient descent methods
for ICA. Another advantage of the FastICA algorithm is that it can be
used to estimate the independent components one-by-one, as in
projection pursuit, which is very practical in exploratory data
analysis. 
<p>
The FastICA package for MATLAB (versions 5 or 4) is freeware package
with a graphical user interface that implements the 
fixed-point algorithm for ICA. The package is available on the Web at
<A href="http://www.cis.hut.fi/projects/ica/fastica/">
http://www.cis.hut.fi/projects/ica/fastica/</A>.<br>
Email contact: Aapo Hyvarinen &lt;Aapo.Hyvarinen@hut.fi&gt;
<p>
<! Aapo Hyvarinen
   
   Helsinki University of Technology
   Lab. of Computer and Information Science
   P.O.Box 5400, FIN-02015 TKK, Finland
   Tel: +358-9-4513278, Fax: +358-9-4513277
   Email: Aapo.Hyvarinen@hut.fi
   Home page: http://www.cis.hut.fi/~aapo/
>



<li><h3><A name="NEXUS">NEXUS: Large-scale biological simulations</a></h3>

Large-scale biological neural network simulation engine. Includes
automated network construction tool that allows extremely complex
networks to be generated according to user-supplied architectural
specifications.
<p>
The network engine is an attempt at creating a biological neural network
simulator. It consists of a C++ class, called "network". A network
object houses a set of objects of another C++ class, called "neuron".
The neuron class is a detailed functional simulation of a neuron (i.e.
the actual chemical processes that lead to a biological neuron's
behavior are not modeled explicitly, but the behavior itself is). The
simulation of the neuron is handled entirely by the neuron class. The
network class coordinates the functioning of the neurons that make up
the neural network, as well as providing addressing services that allow
the neurons to interact. It is also responsible for facilitating the
interface of the neural network it houses onto any existing software
into which the neural network is to be integrated.
<p>
Since a simulated neural network consisting of a large number of heavily
interconnected neurons is extremely difficult to generate manually,
NEXUS was developed. To create a network with NEXUS, one need only
describe the network in general terms, in terms of groups of sets of
specifically arranged neurons, and how the groups interface onto each
other and onto themselves. This information constitutes a network
architecture descriptor. A network architecture descriptor is read by
NEXUS, and NEXUS uses the information to generate a network, building
all the neurons and connecting them together appropriately. This system
is analogous to nature's brain construction system. For example, human
brains, in general, are very similar. The basic design is stored in
human DNA. Since it is certainly not possible to record information
about each neuron and its connections, DNA must instead contain (in some
form) what is essentially a set of guidelines, a set of rules about how
the brain is to be laid out. These guidelines are used to build the
brain, just like NEXUS uses the guidelines set out in the network
architecture descriptor to build the simulated neural network.
<p>
NEXUS and the network engine have deliberately been engineered to be
highly efficient and very compact. Even so, large, complex networks
require tremendous amounts of memory and processing power.
<p>

The network engine:
<ul>
    <li>flexible and elegant design; highly
        customizable simulation parameters; extremely efficient</li>
    <li>throughout, nonlinear magnitude decay
        modeling</li>
    <li>dendritic tree complexity and network
        connection density limited only by the computer hardware</li>
    <li>simulation of dendritic logic gate
        behaviors via a sophisticated excitation thresholding and
        conduction model</li>
    <li>detailed simulation of backprop,
        allowing realistic simulation of associated memory
        formation processes</li>
    <li>simulation of all known postsynaptic
        memory formation mechanisms (STP, STD, LTP, LTD)</li>
    <li>dynamic presynaptic output pattern
        modeling, including excitation magnitude dependent output
        pattern selection</li>
    <li>simulation of all known presynaptic
        activity-based output modifiers (PPF, PTP, depression)</li>
</ul>
<p>

NEXUS:
<ul>
    <li>allows networks to be designed
        concisely and as precisely as is necessary</li>
    <li>makes massively complex large-scale
        neural network design and construction possible</li>
    <li>allows existing networks to be
        augmented without disturbing existing network structure</li>
    <li>UNIX and Win32 compatible</li>
</ul>
<p>

URL: <a href="http://www.sfu.ca/~loryan/neural.html">http://www.sfu.ca/~loryan/neural.html</a><br>
Email: Lawrence O. Ryan &lt;loryan@sfu.ca&gt<p>


<li><h3><A name="Netlab">Netlab: Neural network software for Matlab</a></h3>

<a href="http://www.ncrg.aston.ac.uk/netlab/index.html">http://www.ncrg.aston.ac.uk/netlab/index.html</a>
<p>

The Netlab simulation software is designed to provide the central tools
necessary for the simulation of theoretically well founded neural
network algorithms for use in teaching, research and applications
development.  It consists of a library of Matlab functions and scripts
based on the approach and techniques described in Neural Networks for
Pattern Recognition by Christopher M. Bishop, (Oxford University Press,
1995). The functions come with on-line help, and further explanation is
available via HTML files. <p>

The Netlab library includes software implementations of a wide range of
data analysis techniques. Netlab works with Matlab version 5.0 and
higher. It is not compatible with earlier versions of Matlab. <p>


<li><H3><A name="NuTank"> NuTank</a></H3>

NuTank stands for NeuralTank. It is educational and entertainment
software.  In this program one is given the shell of a 2 dimentional
robotic tank.  The tank has various I/O devices like wheels, whiskers,
optical sensors, smell, fuel level, sound and such.  These I/O sensors
are connected to Neurons.  The player/designer uses more Neurons to
interconnect the I/O devices.  One can have any level of complexity
desired (memory limited) and do subsumptive designs. More complex
design take slightly more fuel, so life is not free. All movement
costs fuel too.  One can also tag neuron connections as "adaptable"
that adapt their weights in acordance with the target neuron. This
allows neurons to learn.  The Neuron editor can handle 3 dimention
arrays of neurons as single entities with very flexible interconect
patterns.<p>

One can then design a scenario with walls, rocks, lights, fat (fuel)
sources (that can be smelled) and many other such things.  Robot tanks
are then introduced into the Scenario and allowed interact or battle
it out.  The last one alive wins, or maybe one just watches the motion
of the robots for fun.  While the scenario is running it can be
stopped, edited, zoom'd, and can track on any robot.<p>

The entire program is mouse and graphicly based.  It uses DOS and VGA
and is written in TurboC++.  There will also be the ability to
download designs to another computer and source code will be available
for the core neural simulator.  This will allow one to design neural
systems and download them to real robots.  The design tools can handle
three dimentional networks so will work with video camera inputs and
such. <p>  

NuTank source code is free from 
<a href="http://www.xmission.com/~rkeene/NuTankSrc.ZIP">http://www.xmission.com/~rkeene/NuTankSrc.ZIP</a><br>
Contact: Richard Keene; Keene Educational Software<br>
Email: rkeene@xmission.com or r.keene@center7.com<p>



<li><H3><A name="Lens"> Lens</a></H3>

<A HREF="http://www.cs.cmu.edu/~dr/Lens">http://www.cs.cmu.edu/~dr/Lens</A><br>
<p>

Lens (the light, efficient network simulator) is a fast, flexible, and
customizable neural network package written primarily in C.  It
currently handles standard backpropagation networks, simple recurrent
(including Jordan and Elman) and fully recurrent nets, deterministic
Boltzmann machines, self-organizing maps, and interactive-activation
models. <p>

Lens runs under Windows as well as a variety of Unix platforms.  It
includes a graphical interface and an embedded script language (Tcl). 
The key to the speed of Lens is its use of tight inner-loops that
minimize memory references when traversing links.  Frequently accessed
values are stored in contiguous memory to achieve good cache
performance.
It is also able to do batch-level parallel training on multiple
processors. <p>

Because it is recognized that no simulator will satisfy sophisticated
users out of the box, Lens was designed to facilitate code
modification.  Users can create and register such things as new network
or group types, new weight update algorithms, or new shell commands
without altering the main body of code.  Therefore, modifications can be
easily transferred to new releases. <p>

Lens is available free-of-charge to those conducting research at
academic or non-profit institutions.  Other users should contact
Douglas Rohde for licensing information at dr+lens@cs.cmu.edu. <p>





<li><H3><A name="Joone"> Joone: Java Object Oriented Neural Engine</a></H3>

   <a href="http://sourceforge.net/projects/joone">
   http://sourceforge.net/projects/joone</a>
   <p>

   Joone is a neural net engine written in Java. It's a modular,
   scalable, multitasking and extensible engine. It can be extended
   by writing new modules to implement new algorithms or new
   architectures starting from simple base components.  It's an Open
   Source project and everybody can contribute to its development. <p>

   Contact: Paolo Marrone, paolo@marrone.org <p>


<li><H3><A name="NV"> NV: Neural Viewer</a></H3>

   <a href="http://www.btinternet.com/~cfinnie/">
   http://www.btinternet.com/~cfinnie/</a>
   <p>

   A free software application for modelling and visualizing complex
   recurrent neural networks in 3D. <p>

<li><H3><A name="EasyNN"> EasyNN</a></H3>

   URL: <a href="http://www.easynn.com/">
   http://www.easynn.com/</a>
   <p>

   EasyNN is a neural network system for Microsoft Windows. It can generate
   multi layer neural networks from text files or grids with minimal user
   intervention. The networks can then be trained, validated and queried.
   Network diagrams, graphs, input/output data and all the network details
   can be displayed and printed. Nodes can be added or deleted while the
   network is learning. The graph, grid, network and detail displays are
   updated dynamically so you can see how the neural networks work.
   EasyNN runs on Windows 95, 98, ME, NT 4.0, 2000 or XP. <p>

<li><H3><A name="MLP-JAVA"> Multilayer Perceptron - A Java Implementation </a></h3>

   Download java from: <a href="http://www.geocities.com/aydingurel/neural/">
   http://www.geocities.com/aydingurel/neural/</a>
   <p>

   What can you exactly do with it? You can:
   <ul>
   <li> Build nets with any number of layers and units. Layers are connected to each other consecutively, each unit in a layer is connected to all of the units on the next layer (and vice versa) if there is one,
   <li> Set units with linear and sigmoid activation functions and set them separately for each layer,
   <li> Set parameters for sigmoid functions and set them separately for each layer,
   <li> Use momentum, set different momentum parameters for each layer,
   <li> Initialize the net using your own set of weights,
   <li> Train the net using backpropagation and with any training rate.
   </ul>
   <p>

   Contact: Aydin Gurel, aydin.gurel@lycos.com  <p>   

</ol>

<pre>
------------------------------------------------------------------------
</pre>

<p>
For some of these simulators there are user mailing lists. Get the
packages and look into their documentation for further info.<p>

<pre>
------------------------------------------------------------------------
</pre>

Next part is <A HREF="FAQ6.html">part 6</A> (of 7).
Previous part is <A HREF="FAQ4.html">part 4</A>.
